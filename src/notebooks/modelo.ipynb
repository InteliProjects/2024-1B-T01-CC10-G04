{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmDm2ar9woWj"
      },
      "source": [
        "# Sprint 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEnDEk72qAUM"
      },
      "source": [
        "## 1. Descrição da fonte de dados com justificativa para escolha da base de dados\n",
        "\n",
        "## Descrição da Plataforma Sentinel Hub\n",
        "\n",
        "O **Sentinel Hub** é uma plataforma baseada em nuvem projetada para facilitar o acesso, o processamento e a análise de grandes quantidades de dados de observação da Terra. Ela suporta imagens de uma variedade de satélites, incluindo os satélites Sentinel, parte do programa Copernicus da União Europeia. Estes satélites fornecem dados atualizados regularmente, cobrindo uma ampla gama de bandas espectrais, essenciais para aplicações que vão desde monitoramento agrícola até gestão de desastres naturais.\n",
        "\n",
        "### Principais Recursos e Funcionalidades\n",
        "\n",
        "- **Acesso Facilitado aos Dados**: O Sentinel Hub oferece acesso simplificado a um vasto acervo de dados de satélite, permitindo análises em tempo real e históricas.\n",
        "- **Processamento na Nuvem**: A capacidade de processar dados diretamente na nuvem elimina a necessidade de infraestrutura local pesada, tornando a plataforma acessível para organizações de todos os portes.\n",
        "- **APIs Flexíveis**: Com APIs robustas, o Sentinel Hub permite a automatização do acesso e processamento de dados, facilitando a integração com pipelines de dados e aplicativos existentes.\n",
        "- **Customização de Processamento de Imagens**: Usuários podem personalizar scripts de processamento para extrair informações específicas das imagens, adequadas a necessidades particulares de cada projeto.\n",
        "- **Atualizações Frequentes e Histórico de Dados**: O serviço mantém um histórico extenso e realiza atualizações frequentes, crucial para estudos de longo prazo e monitoramento contínuo.\n",
        "\n",
        "### Vantagens do Uso do Sentinel Hub como Base de Dados\n",
        "\n",
        "- **Escalabilidade**: A arquitetura baseada em nuvem do Sentinel Hub suporta o processamento de grandes volumes de dados de maneira eficiente.\n",
        "- **Flexibilidade e Integração**: As APIs facilitam a integração com outros sistemas e ferramentas, suportando uma arquitetura de sistema diversificada.\n",
        "- **Custo-Efetividade**: Reduzindo a necessidade de infraestrutura de hardware e manutenção, o Sentinel Hub oferece uma solução econômica para processamento e análise de dados.\n",
        "- **Acesso Democrático a Dados de Alta Qualidade**: A combinação de acesso aberto aos dados do Copernicus e as ferramentas avançadas do Sentinel Hub democratiza o acesso a informações de alta qualidade para uma vasta gama de usuários.\n",
        "\n",
        "Portanto, concluímos que o Sentinel Hub se destaca como uma ótima escolha para a segmentação de talhões por meio de visão computacional, pois nos permite um acesso rápido, processamento eficiente e análises profundas de dados de observação da Terra, através de uma plataforma robusta e versátil.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdwCvhGevUuS"
      },
      "source": [
        "### Descrição da escolha de imagens na base de dados do Sentinel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6s5TsDVvUuS"
      },
      "source": [
        "![Relevo da Região Sul](./assets/img/sentinel-image1-regiao-sul.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgGiW2TDvUuT"
      },
      "source": [
        "A primeira imagem exibida retrata uma porção de terra na região do Rio Grande do Sul, evidenciando uma distribuição considerável de talhões agrícolas. Observa-se que a maioria dos talhões exibe uma forma retangular, essa é uma característica comum em práticas de agricultura de larga escala. Entretanto, é visto que esses talhões possuem dimensões reduzidas (são diversos talhões em uma única imagem), o que pode acarretar dificuldades na identificação individual de cada unidade de cultivo. Essa alta densidade e extensão dos talhões contribuem para a complexidade do processo de identificação.\n",
        "\n",
        "A abordagem proposta visa utilizar esta imagem como uma das muitas para treinar a rede neural a reconhecer os talhões agrícolas. A inclusão de uma quantidade significativa de talhões na imagem é estratégica, pois proporciona à rede neural uma exposição mais abrangente a diferentes configurações e padrões de talhões. A preferência por uma maior quantidade de talhões na imagem decorre da necessidade de garantir que a rede neural seja capaz de identificar os talhões de forma precisa e individualizada, minimizando tanto os falsos positivos quanto os falsos negativos.\n",
        "\n",
        "Os falsos positivos ocorrem quando a rede neural identifica erroneamente um objeto como sendo um talhão, enquanto os falsos negativos ocorrem quando a rede deixa de identificar um talhão existente na imagem. Ao expor a rede neural a uma variedade de cenários com um grande número de talhões, busca-se mitigar a incidência desses erros (principalmente os falsos negativos), possibilitando um treinamento mais eficiente e preciso da rede para a tarefa de identificação de talhões agrícolas. A consideração desses conceitos de falsos positivos e falsos negativos é essencial para garantir a robustez e a confiabilidade do modelo de identificação de talhões agrícolas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StAmMk3TvUuT"
      },
      "source": [
        "![Relevo da Região Sul](./assets/img/sentinel-image2-regiao-sul.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aAQpgYOvUuT"
      },
      "source": [
        "A segunda imagem exibida retrata uma porção de terra na região do Rio Grande do Sul, evidenciando uma distribuição reduzida de talhões agrícolas em comparação com a imagem anterior. No entanto, observa-se uma maior diversidade de tamanhos e formatos de talhões presentes nesta imagem. Ao contrário da imagem anterior, onde a maioria dos talhões apresentava uma forma retangular e dimensões relativamente uniformes, nesta imagem os talhões exibem uma variedade de formas e tamanhos, incluindo formas irregulares e tamanhos variados.\n",
        "\n",
        "A abordagem proposta visa utilizar esta imagem para expandir o conjunto de dados de treinamento do modelo, permitindo que ele seja exposto a uma maior diversidade de padrões de talhões. A inclusão de talhões com diferentes tamanhos e formatos é estratégica, pois capacita o modelo a reconhecer e identificar talhões que possuam características distintas dos padrões convencionais. Isso é fundamental para garantir que o modelo seja capaz de lidar eficientemente com a variedade de talhões encontrados na prática agrícola, incluindo talhões com formatos irregulares e tamanhos não padronizados.\n",
        "\n",
        "\n",
        "A ideia da base de dados é visa criar um conjunto diversificado de imagens representativas dos cenários agrícolas no Rio Grande do Sul. Isso inclui diferentes tipos de culturas, condições de solo e práticas de cultivo. A seleção estratégica das imagens considera a variedade de padrões de talhões, abrangendo tamanhos, formas e arranjos diversos. Durante o treinamento, o modelo aprende com exemplos anteriores, identificando padrões visuais para associar com a presença de talhões, garantindo sua capacidade de identificar talhões com precisão em novas imagens. Esse processo permite o desenvolvimento de um modelo robusto e preciso de identificação de talhões agrícolas na região sul.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdDOeYlVqJkx"
      },
      "source": [
        "## 2. Pipeline de processamento e preparação de dados de imagens para treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLoluWVsvUuU",
        "outputId": "3c85c2ed-9c09-4f81-f5b6-9fef310e52e1"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHxrOvHZvUuU",
        "outputId": "5ee8c1f0-0c9a-43e0-d822-5342bde83b11"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BCKkGMivUuV"
      },
      "source": [
        "### 2.1 Descompactando pastas zipadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8I5Clk8pm3K",
        "outputId": "9b821dbd-7d07-4fe8-e79b-3e9e26f77829"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Lista os arquivos e diretórios na pasta atual\n",
        "conteudos = os.listdir('.')\n",
        "\n",
        "# Imprime os conteúdos\n",
        "for item in conteudos:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1vKlEV6UvUuV",
        "outputId": "f48382cb-7019-4cf4-853b-4dbe05c0aa98"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Caminho para a pasta onde estão os arquivos zip\n",
        "path = 'data/'\n",
        "\n",
        "# Lista todos os arquivos na pasta especificada\n",
        "files = os.listdir(path)\n",
        "\n",
        "# Filtra e descompacta apenas os arquivos zip\n",
        "for file in files:\n",
        "    if file.endswith('.zip'):\n",
        "        zip_path = os.path.join(path, file)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(path)\n",
        "        print(f'Descompactado: {file}')\n",
        "\n",
        "# Remove os arquivos zip após descompactá-los\n",
        "for file in files:\n",
        "    if file.endswith('.zip'):\n",
        "        os.remove(os.path.join(path, file))\n",
        "        print(f'Removido: {file}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO_qxfUNvUuV"
      },
      "source": [
        "### 2.2 Pipeline de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D77NKfLMpthy"
      },
      "outputs": [],
      "source": [
        "# Importing useful libraries\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from osgeo import gdal\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import generic_filter\n",
        "from scipy import ndimage\n",
        "from scipy.stats import mode\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv_IUzf6v7Dr"
      },
      "source": [
        "### Função para Carregar e Redimensionar Bandas Espectrais\n",
        "\n",
        "A função load_and_resize_images carrega e redimensiona imagens .tif em um diretório especificado, ajustando-as para um tamanho padrão (1200, 1200) para garantir consistência na análise de imagens de satélite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10IhYhEEpu0h"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Helper function to load and resize spectral bands\n",
        "def load_and_resize_images(folder_path, target_size=(1200, 1200)):\n",
        "    images = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".tif\"):\n",
        "            band = filename.split('.')[0]\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = gdal.Open(img_path)\n",
        "            array = img.ReadAsArray()\n",
        "            resized = cv2.resize(array, target_size, interpolation=cv2.INTER_LINEAR)\n",
        "            images[band] = resized\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD1ew-CewCy6"
      },
      "source": [
        "\n",
        "### Função para Correção Atmosférica\n",
        "\n",
        "A função `atmospheric_correction` aplica uma correção radiométrica simples a arrays de bandas de imagens, dividindo cada banda pelo seu valor máximo para minimizar efeitos atmosféricos e melhorar a análise de dados de satélite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqZjlHvdpxiz"
      },
      "outputs": [],
      "source": [
        "# Atmospheric correction to remove atmospheric effects\n",
        "def atmospheric_correction(band_array):\n",
        "    # Simple example of a radiometric correction\n",
        "    corrected_band = band_array / (band_array.max() + 1e-10)\n",
        "    return corrected_band\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StSGMx2xwONN"
      },
      "source": [
        "\n",
        "### Função para Calcular Índices de Vegetação\n",
        "\n",
        "A função `calculate_vegetation_indices` calcula índices de vegetação, incluindo NDVI, EVI e GNDVI, utilizando bandas espectrais específicas de imagens de satélite. A função garante a presença de todas as bandas necessárias e aplica correções baseadas em valores específicos para melhorar a identificação e análise da vegetação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a5KkmjIp0AH"
      },
      "outputs": [],
      "source": [
        "def calculate_vegetation_indices(images):\n",
        "    required_bands = ['b8', 'b4', 'b2', 'b3']\n",
        "    if all(band in images for band in required_bands):\n",
        "        nir = images['b8'].astype(float)\n",
        "        red = images['b4'].astype(float)\n",
        "        green = images['b3'].astype(float)\n",
        "        blue = images['b2'].astype(float)\n",
        "\n",
        "        # Calculating NDVI\n",
        "        ndvi = (nir - red) / (nir + red + 1e-10)\n",
        "\n",
        "\n",
        "        # Calculating GNDVI and applying thresholds\n",
        "        gndvi = (nir - green) / (nir + green + 1e-10)\n",
        "\n",
        "\n",
        "        return {'NDVI': ndvi, 'GNDVI': gndvi}\n",
        "    else:\n",
        "        missing = [band for band in required_bands if band not in images]\n",
        "        print(f\"Missing required bands: {missing}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4P_lOR2xn2f"
      },
      "source": [
        "\n",
        "### Função para Processar Imagens de Satélite\n",
        "\n",
        "A função `process_images` automatiza o processamento das imagens de satélite.\n",
        "\n",
        "1. **Carregamento e Redimensionamento de Imagens**: As imagens são carregadas e redimensionadas para um tamanho padrão a partir dos diretórios especificados.\n",
        "2. **Correção Atmosférica**: Aplica uma correção atmosférica básica em cada banda espectral das imagens.\n",
        "3. **Cálculo de Índices de Vegetação**: Calcula índices como NDVI a partir das bandas corrigidas.\n",
        "7. **Visualização**:\n",
        "   - Exibe as imagens RGB filtradas e suas bordas para avaliação visual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIb2XSS0GD2K"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_images():\n",
        "    data_dir = \"./data/dataset_inteli/images\"  # Base directory for spectral images\n",
        "    rgb_dir = \"./data/dataset_inteli/tci_pngs\"  # Directory containing RGB images\n",
        "\n",
        "    for folder_name in sorted(os.listdir(data_dir)):\n",
        "        folder_path = os.path.join(data_dir, folder_name)\n",
        "        if os.path.isdir(folder_path) and folder_name != '.ipynb_checkpoints':\n",
        "            print(f\"Processing folder: {folder_path}\")\n",
        "            images = load_and_resize_images(folder_path)\n",
        "\n",
        "            if not all(band in images for band in ['b4', 'b8', 'b2', 'b3']):\n",
        "                print(f\"Missing required bands in {folder_name}, skipping this folder.\")\n",
        "                continue\n",
        "\n",
        "            # Process vegetation indices and segmentation\n",
        "            corrected_images = {band: atmospheric_correction(images[band]) for band in images}\n",
        "            indices = calculate_vegetation_indices(corrected_images)\n",
        "            normalized = cv2.normalize(indices, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "\n",
        "            # Visualization\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(cv2.cvtColor(normalized, cv2.COLOR_BGR2RGB))\n",
        "            plt.title('Filtered RGB Image')\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(edges, cmap='gray')\n",
        "            plt.title('Edges of Filtered Image')\n",
        "            plt.show()\n",
        "\n",
        "process_images()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kI7vI3QqW1R"
      },
      "source": [
        "## 3. Base de dados de imagens processadas com uma análise exploratória sobre a base obtida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf-_u1POvUuV"
      },
      "outputs": [],
      "source": [
        "pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT2NSKxcvUuW"
      },
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import display, Image\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQDaXjRkvUuW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def display_images(directory):\n",
        "\n",
        "    \"\"\"\n",
        "    Exibe três imagens aleatórias de um diretório.\n",
        "\n",
        "    Args:\n",
        "    - directory: String. Caminho do diretório onde as imagens estão armazenadas.\n",
        "    \"\"\"\n",
        "\n",
        "    images = [img for img in os.listdir(directory) if img.endswith('.png')]\n",
        "    selected_images = random.sample(images, min(len(images), 3))  # Escolhe 3 imagens aleatórias\n",
        "\n",
        "    for img_name in selected_images:\n",
        "        img_path = os.path.join(directory, img_name)\n",
        "        print(f\"Displaying Image: {img_name}\")\n",
        "        display(Image(filename=img_path))\n",
        "\n",
        "display_images('./data/dataset_inteli/tci_pngs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMSGp8QzvUuW"
      },
      "outputs": [],
      "source": [
        "def plot_brightness_distribution(directory):\n",
        "\n",
        "    \"\"\"\n",
        "    Plota a distribuição do brilho das imagens.\n",
        "\n",
        "    Args:\n",
        "    - directory: String. Caminho do diretório onde as imagens estão armazenadas.\n",
        "    \"\"\"\n",
        "\n",
        "    brightness_values = []\n",
        "\n",
        "    for img_file in os.listdir(directory):\n",
        "        if img_file.endswith('.png'):\n",
        "            img_path = os.path.join(directory, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "            brightness = hsv_img[:, :, 2].flatten()\n",
        "            brightness_values.extend(brightness)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(brightness_values, bins=256, color='gold', alpha=0.7)\n",
        "    plt.title('Distribuição de Brilho')\n",
        "    plt.xlabel('Brilho')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.show()\n",
        "\n",
        "plot_brightness_distribution('./data/dataset_inteli/tci_pngs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYr8eLKZvUuW"
      },
      "outputs": [],
      "source": [
        "def plot_color_histograms(directory):\n",
        "\n",
        "    \"\"\"\n",
        "    Calcula e plota os histogramas de cores para os canais RGB.\n",
        "\n",
        "    Args:\n",
        "    - directory: Caminho do diretório onde as imagens estão armazenadas.\n",
        "    \"\"\"\n",
        "\n",
        "    color_data = {'Red': [], 'Green': [], 'Blue': []}\n",
        "\n",
        "    for img_name in os.listdir(directory):\n",
        "        if img_name.endswith('.png'):\n",
        "            img_path = os.path.join(directory, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                for i, color in enumerate(['Blue', 'Green', 'Red']):\n",
        "                    hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
        "                    color_data[color].extend(hist.flatten())\n",
        "\n",
        "    # Plota os histogramas de cor\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, color in enumerate(['Blue', 'Green', 'Red']):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.plot(color_data[color], color=color.lower())\n",
        "        plt.title(f'{color} Channel Histogram')\n",
        "        plt.xlabel('Intensidade de Cor')\n",
        "        plt.ylabel('Frequência')\n",
        "        plt.xlim([0, 256])\n",
        "    plt.show()\n",
        "\n",
        "plot_color_histograms('./data/dataset_inteli/tci_pngs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL7aN5frvUuW"
      },
      "outputs": [],
      "source": [
        "def plot_pixel_intensity_distribution(directory):\n",
        "\n",
        "    \"\"\"\n",
        "    Gera um histograma da distribuição de intensidade de pixel das imagens.\n",
        "\n",
        "    Args:\n",
        "    - directory: Caminho do diretório onde as imagens estão armazenadas.\n",
        "    \"\"\"\n",
        "\n",
        "    intensidades = []\n",
        "    # Itera sobre cada arquivo no diretório especificado\n",
        "    for img_name in os.listdir(directory):\n",
        "        if img_name.endswith('.png'):\n",
        "            img_path = os.path.join(directory, img_name)\n",
        "            # Carrega a imagem em escala de cinza\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if img is not None:\n",
        "                # Extrai e armazena todos os valores de pixel da imagem\n",
        "                intensidades.extend(img.ravel())\n",
        "\n",
        "    # Cria um histograma com a distribuição dos valores de pixel\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(intensidades, bins=256, color='gray', alpha=0.7)\n",
        "    plt.title('Distribuição de Intensidade de Pixel')\n",
        "    plt.xlabel('Intensidade')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.show()\n",
        "\n",
        "plot_pixel_intensity_distribution('./data/dataset_inteli/tci_pngs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTIG_wbuvUuX"
      },
      "outputs": [],
      "source": [
        "def plot_color_channel_correlation(directory):\n",
        "\n",
        "    \"\"\"\n",
        "    Gera um mapa de calor da correlação entre os canais de cores das imagens.\n",
        "\n",
        "    Args:\n",
        "    - directory: Caminho do diretório onde as imagens estão armazenadas.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "    # Itera sobre cada arquivo no diretório\n",
        "    for img_name in os.listdir(directory):\n",
        "        if img_name.endswith('.png'):\n",
        "            img_path = os.path.join(directory, img_name)\n",
        "            # Carrega a imagem em colorido\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                # Calcula e armazena as médias dos valores dos canais RGB\n",
        "                mean_colors = cv2.mean(img)[:3]  # Ignora o canal alpha se houver\n",
        "                data.append(mean_colors)\n",
        "\n",
        "    # Cria um DataFrame com os dados coletados\n",
        "    df = pd.DataFrame(data, columns=['Blue', 'Green', 'Red'])\n",
        "    # Calcula a matriz de correlação\n",
        "    correlation_matrix = df.corr()\n",
        "\n",
        "    # Visualiza a matriz de correlação com um mapa de calor\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlação entre Canais de Cores')\n",
        "    plt.show()\n",
        "\n",
        "plot_color_channel_correlation('./data/dataset_inteli/tci_pngs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LAGZ1xCvUuX"
      },
      "outputs": [],
      "source": [
        "def plot_saturation_distribution(directory):\n",
        "\n",
        "    \"\"\"\n",
        "    Plota a distribuição da saturação das imagens.\n",
        "\n",
        "    Args:\n",
        "    - directory: String. Caminho do diretório onde as imagens estão armazenadas.\n",
        "    \"\"\"\n",
        "\n",
        "    saturation_values = []\n",
        "\n",
        "    for img_file in os.listdir(directory):\n",
        "        if img_file.endswith('.png'):\n",
        "            img_path = os.path.join(directory, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "            saturation = hsv_img[:, :, 1].flatten()\n",
        "            saturation_values.extend(saturation)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(saturation_values, bins=256, color='green', alpha=0.7)\n",
        "    plt.title('Distribuição de Saturação')\n",
        "    plt.xlabel('Saturação')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.show()\n",
        "\n",
        "plot_saturation_distribution('./data/dataset_inteli/tci_pngs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr2KlowMvUuX"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def plot_color_variety_with_pca(directory):\n",
        "\n",
        "    \"\"\"\n",
        "    Aplica PCA para visualizar a variedade de cores em um conjunto de imagens.\n",
        "\n",
        "    Args:\n",
        "    - directory: String. Caminho do diretório onde as imagens estão armazenadas.\n",
        "    \"\"\"\n",
        "\n",
        "    color_data = []\n",
        "\n",
        "    for img_file in os.listdir(directory):\n",
        "        if img_file.endswith('.png'):\n",
        "            img_path = os.path.join(directory, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            color_data.append(img.reshape(-1, 3))\n",
        "\n",
        "    color_data = np.vstack(color_data)\n",
        "    scaler = StandardScaler()\n",
        "    color_data_normalized = scaler.fit_transform(color_data)\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    principal_components = pca.fit_transform(color_data_normalized)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.2)\n",
        "    plt.title('Visualização da Variedade de Cores com PCA')\n",
        "    plt.xlabel('Primeiro Componente Principal')\n",
        "    plt.ylabel('Segundo Componente Principal')\n",
        "    plt.show()\n",
        "\n",
        "plot_color_variety_with_pca('./data/dataset_inteli/tci_pngs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9-0H0g6yYaF"
      },
      "source": [
        "# Sprint 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mATEhUHWTvDp"
      },
      "source": [
        "## Conectando ao Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqwe4P_CLSrJ",
        "outputId": "3f286839-57cc-4ecd-add9-6a2151297bb6"
      },
      "outputs": [],
      "source": [
        "# Mounting GDrive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSykBC-bshpp"
      },
      "source": [
        "## Import das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSn5gXSo6RFH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBmWgHklsqVt"
      },
      "source": [
        "## Preparação dos Dados de Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIZjTk2TtlsW"
      },
      "source": [
        "Separação dos dados para treino do modelo. A ideia foi separar em diretórios distintos, cada variável vai armazenar o diretório com um tipo específico de imagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSc1XAWu0HGB"
      },
      "outputs": [],
      "source": [
        "# Importando as imagens pelo diretório de categoria de subpasta\n",
        "\n",
        "# Dados de treino do modelo\n",
        "\n",
        "addressDSmain = '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli'\n",
        "addressMasks = '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/masks'\n",
        "addressPngs = '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/tci_pngs'\n",
        "addressTifs = '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/tci_tifs'\n",
        "addressMarkedRgbs = '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/marked_rgbs'\n",
        "addressRgbs = '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/rgbs'\n",
        "\n",
        "treinamento_dir = os.path.join(addressDSmain)\n",
        "treinamento_masks = os.path.join(addressMasks)\n",
        "treinamento_pngs = os.path.join(addressPngs)\n",
        "treinamento_tifs = os.path.join(addressTifs)\n",
        "treinamento_marked_rgbs = os.path.join(addressMarkedRgbs)\n",
        "treinamento_rgbs = os.path.join(addressRgbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUbgH1Gh6azS"
      },
      "outputs": [],
      "source": [
        "treinamento_dir_nomes = sorted(os.listdir(treinamento_dir))\n",
        "treinamento_masks_nomes = sorted(os.listdir(treinamento_masks))\n",
        "treinamento_pngs_nomes = sorted(os.listdir(treinamento_pngs))\n",
        "treinamento_tifs_nomes = sorted(os.listdir(treinamento_tifs))\n",
        "treinamento_marked_rgbs_nomes = sorted(os.listdir(treinamento_marked_rgbs))\n",
        "treinamento_rgbs_nomes = sorted(os.listdir(treinamento_rgbs))\n",
        "\n",
        "diretorios_desejados = ['masks', 'marked_rgbs', 'rgbs', 'tci_pngs', 'tci_tifs']\n",
        "\n",
        "# Filtrar a lista mantendo apenas os diretórios desejados\n",
        "treinamento_dir_nomes = list(filter(lambda x: x in diretorios_desejados, treinamento_dir_nomes))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uO8sm6Wo7VH",
        "outputId": "a4f0eb3f-6c13-4e86-db49-ec4495581370"
      },
      "outputs": [],
      "source": [
        "print(treinamento_dir_nomes)\n",
        "print(treinamento_marked_rgbs_nomes[:5])\n",
        "print(treinamento_masks_nomes[:5])\n",
        "print(treinamento_rgbs_nomes[:5])\n",
        "print(treinamento_pngs_nomes[:5])\n",
        "print(treinamento_tifs_nomes[:5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkh3qQJwpky6"
      },
      "source": [
        "## Preparação dos Dados de Teste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1gFcxm57_6h"
      },
      "outputs": [],
      "source": [
        "## Dados de Teste\n",
        "\n",
        "addressDSmainTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test'\n",
        "addressMasksTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/masks'\n",
        "addressPngsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/tci_pngs'\n",
        "addressTifsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/tci_tifs'\n",
        "addressMarkedRgbsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/marked_rgbs'\n",
        "addressRgbsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/rgbs'\n",
        "\n",
        "treinamento_dir_teste = os.path.join(addressDSmainTeste)\n",
        "treinamento_masks_teste = os.path.join(addressMasksTeste)\n",
        "treinamento_pngs_teste = os.path.join(addressPngsTeste)\n",
        "treinamento_tifs_teste = os.path.join(addressTifsTeste)\n",
        "treinamento_marked_rgbs_teste = os.path.join(addressMarkedRgbsTeste)\n",
        "treinamento_rgbs_teste = os.path.join(addressRgbsTeste)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t8juFQD7_0P",
        "outputId": "5d09f56a-4fd2-42c8-eb0f-aeddbc8ceabd"
      },
      "outputs": [],
      "source": [
        "# Dados de Teste\n",
        "\n",
        "treinamento_dir_teste_nomes = sorted(os.listdir(treinamento_dir_teste))\n",
        "treinamento_masks_teste_nomes = sorted(os.listdir(treinamento_masks_teste))\n",
        "treinamento_pngs_teste_nomes = sorted(os.listdir(treinamento_pngs_teste))\n",
        "treinamento_tifs_teste_nomes = sorted(os.listdir(treinamento_tifs_teste))\n",
        "treinamento_marked_rgbs_teste_nomes = sorted(os.listdir(treinamento_marked_rgbs_teste))\n",
        "treinamento_rgbs_teste_nomes = sorted(os.listdir(treinamento_rgbs_teste))\n",
        "\n",
        "diretorios_desejados = ['masks', 'marked_rgbs', 'rgbs', 'tci_pngs', 'tci_tifs']\n",
        "\n",
        "# Filtrar a lista mantendo apenas os diretórios desejados\n",
        "treinamento_dir_teste_nomes_filtrados = list(filter(lambda x: x in diretorios_desejados, treinamento_dir_teste_nomes))\n",
        "\n",
        "treinamento_ValTest = os.path.join(addressDSmainTeste)\n",
        "\n",
        "print(treinamento_dir_teste_nomes_filtrados)\n",
        "print(treinamento_masks_teste_nomes[:3])\n",
        "print(treinamento_pngs_teste_nomes[:3])\n",
        "print(treinamento_tifs_teste_nomes[:3])\n",
        "print(treinamento_marked_rgbs_teste_nomes[:3])\n",
        "print(treinamento_rgbs_teste_nomes[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHfqnUAovpQI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Caminhos para os diretórios de teste\n",
        "addressDSmainTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test'\n",
        "addressMasksTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/masks'\n",
        "addressPngsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/tci_pngs'\n",
        "addressTifsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/tci_tifs'\n",
        "addressMarkedRgbsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/marked_rgbs'\n",
        "addressRgbsTeste = '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/rgbs'\n",
        "\n",
        "# Lista de nomes dos arquivos nos diretórios de teste\n",
        "treinamento_masks_teste_nomes = sorted(os.listdir(addressMasksTeste))\n",
        "treinamento_pngs_teste_nomes = sorted(os.listdir(addressPngsTeste))\n",
        "treinamento_tifs_teste_nomes = sorted(os.listdir(addressTifsTeste))\n",
        "treinamento_marked_rgbs_teste_nomes = sorted(os.listdir(addressMarkedRgbsTeste))\n",
        "treinamento_rgbs_teste_nomes = sorted(os.listdir(addressRgbsTeste))\n",
        "\n",
        "# Dicionário para mapear nomes de diretório para os caminhos\n",
        "diretorios_teste = {\n",
        "    'masks': addressMasksTeste,\n",
        "    'marked_rgbs': addressMarkedRgbsTeste,\n",
        "    'rgbs': addressRgbsTeste,\n",
        "    'tci_pngs': addressPngsTeste,\n",
        "    'tci_tifs': addressTifsTeste\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR-jNFU8vqa6"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tve9-13v1OfK"
      },
      "source": [
        "A ideia é gerar novos dados dentro de cada categoria selecionada, foram criadas novas imagens com diferenças de rotação, corte, ampliação das originais. Elas são geradas dentro do dataset das categorias selecionadas. Havia poucas imagens dentro das categorias e isso gerava um não aprendizado do modelo por falta de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "WXos79CeFBm4",
        "outputId": "3088b3a8-724c-430e-c8b5-148ed554ae1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "directories = [\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/masks',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/tci_pngs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/tci_tifs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/marked_rgbs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli/dataset_inteli/rgbs',\n",
        "\n",
        "\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/masks',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/tci_pngs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/tci_tifs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/marked_rgbs'\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_test/dataset_inteli_test/rgbs'\n",
        "\n",
        "]\n",
        "\n",
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=180,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "def augment_and_save(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            img = load_img(file_path)\n",
        "            img_array = img_to_array(img)\n",
        "            img_array = img_array.reshape((1,) + img_array.shape)\n",
        "\n",
        "            i = 0\n",
        "            for batch in data_gen.flow(img_array, batch_size=1, save_to_dir=directory, save_prefix='aug_', save_format='jpg'):\n",
        "                i += 1\n",
        "                if i >= 2:\n",
        "                    break\n",
        "\n",
        "for directory in directories:\n",
        "    augment_and_save(directory)\n",
        "    print(f\"Transformações aplicadas e 6 imagens salvas em: {directory}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS_7z6HVYEUN",
        "outputId": "01341f23-a4f4-442a-b8f4-bf796e2bdad2"
      },
      "outputs": [],
      "source": [
        "for dir_nome, caminho_dir, arquivos in zip(treinamento_dir_nomes, [addressMarkedRgbs, addressMasks, addressRgbs, addressPngs, addressTifs], [treinamento_marked_rgbs_nomes, treinamento_masks_nomes, treinamento_rgbs_nomes, treinamento_pngs_nomes, treinamento_tifs_nomes]):\n",
        "    for index, arquivo in enumerate(arquivos):  # Não especificamos um valor inicial para index\n",
        "        # Caminho completo do arquivo original\n",
        "        arquivo_original = os.path.join(caminho_dir, arquivo)\n",
        "        # Novo nome do arquivo\n",
        "        novo_nome = f\"{dir_nome}_{index}.png\"\n",
        "        # Caminho completo do novo arquivo\n",
        "        novo_arquivo = os.path.join(caminho_dir, novo_nome)\n",
        "        print(f\"Renomeando {arquivo_original} para {novo_arquivo}\")  # Imprimir o caminho completo antes de renomear\n",
        "        # Renomear o arquivo\n",
        "        os.rename(arquivo_original, novo_arquivo)\n",
        "        # Atualizar a lista de nomes dos arquivos\n",
        "        arquivos[index] = novo_nome\n",
        "\n",
        "print(treinamento_marked_rgbs_nomes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "amTK5jeq0TmW",
        "outputId": "1435d4aa-5cf2-4d92-c703-87792baac2d7"
      },
      "outputs": [],
      "source": [
        "nlinhas = 4\n",
        "ncolunas = 4\n",
        "pic_index_INI = 8\n",
        "size_img = 4\n",
        "# Índice da figura\n",
        "pic_index = 0\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncolunas * size_img, nlinhas * size_img)\n",
        "\n",
        "# Inicializar a variável pic_index com valor final máximo: pic_index_INI\n",
        "pic_index += pic_index_INI\n",
        "\n",
        "proximo_pix = [os.path.join(treinamento_masks, IMGnome)\n",
        "               for IMGnome in treinamento_masks_nomes[pic_index-pic_index_INI:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(proximo_pix):\n",
        "    sp = plt.subplot(nlinhas, ncolunas, i + 1)\n",
        "    sp.axis('Off')\n",
        "    try:\n",
        "        img = mpimg.imread(img_path)\n",
        "        plt.imshow(img)\n",
        "    except Exception as e:\n",
        "        print(f\"Não foi possível carregar a imagem {img_path}: {e}\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOgM7jo4poAu"
      },
      "source": [
        "## Descrição do modelo utilizado\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OyGqQ42t0q2"
      },
      "source": [
        "O modelo escolhido pelo grupo é uma rede neural convolucional (CNN) implementada com as bibliotecas TensorFlow e Keras, adequadas para a tarefa de classificação de imagens. O modelo inicia com uma camada de entrada configurada para imagens de 300x300 pixels com três canais de cor. Seguem-se cinco camadas convolucionais intercaladas com camadas de pooling máximo para redução dimensional, cada uma utilizando filtros de 3x3 e ativação ReLU, aumentando progressivamente o número de filtros de 16 até 64. A rede também inclui uma camada de achatamento seguida por uma camada densa de 128 neurônios com ativação ReLU e uma camada de saída com 5 neurônios utilizando a função de ativação softmax para a classificação multiclasse. Este design é suportado por práticas comuns na literatura de aprendizado profundo, como as discutidas por Goodfellow, Bengio, e Courville (2016) em \"Deep Learning\" e Chollet (2017) em \"Deep Learning with Python\", onde a combinação de convolução, pooling e camadas densas é fundamental para a extração e classificação eficaz de características em imagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0NADfjg_fhn"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgcSbzHe_zyA"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer=RMSprop(learning_rate=0.001),\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_p4h79QjKJ4"
      },
      "outputs": [],
      "source": [
        "# Capturar tempo inicial da execução\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKeoCqTgz8Yl"
      },
      "source": [
        "## Pré-Processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt9Nqj1mtFEZ",
        "outputId": "c35d1448-c5f6-4221-9336-16c2bd5a1e20"
      },
      "outputs": [],
      "source": [
        "# Diretório principal dos dados de teste\n",
        "teste_dir = os.path.join(addressDSmainTeste)\n",
        "\n",
        "# Diretórios considerados para ler dados\n",
        "classes_teste = ['masks', 'marked_rgbs', 'rgbs', 'tci_pngs', 'tci_tifs']\n",
        "\n",
        "# Cria um ImageDataGenerator para pré-processar os dados de teste\n",
        "teste_dadosGerados = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Cria um gerador de dados a partir do diretório principal dos dados de teste, considerando apenas as classes desejadas\n",
        "teste_gerador = teste_dadosGerados.flow_from_directory(\n",
        "    teste_dir,\n",
        "    target_size=(300, 300),\n",
        "    batch_size=32,\n",
        "    classes=classes_teste,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHvWN0eI_6eX",
        "outputId": "581e3211-f41e-4d35-c4a2-6c7b7c148795"
      },
      "outputs": [],
      "source": [
        "# Diretório principal\n",
        "treinamento_dir = os.path.join(addressDSmain)\n",
        "\n",
        "# Diretório de classes usadas\n",
        "classes_treinamento = ['masks', 'marked_rgbs', 'rgbs', 'tci_pngs', 'tci_tifs']\n",
        "\n",
        "# Criando um ImageDataGenerator para pré-processar os dados de treinamento\n",
        "treina_dadosGerados = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Criando um gerador de dados a partir do diretório principal, considerando apenas as classes desejadas\n",
        "treina_gerador = treina_dadosGerados.flow_from_directory(\n",
        "    treinamento_dir,\n",
        "    target_size=(300, 300),\n",
        "    batch_size=128,\n",
        "    classes=classes_treinamento,  # Lista de classes desejadas\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jvAnchKuNqR"
      },
      "source": [
        "## Métricas Geradas pelo treinamento a partir da Execução do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNjrf4iiEwh9",
        "outputId": "32e62fc4-7ee3-4501-dc69-1d5480a31c7e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Calcula o número total de amostras de treinamento\n",
        "total_samples = treina_gerador.samples\n",
        "\n",
        "# Calcula o número de lotes por época para garantir que todos os dados sejam usados\n",
        "batch_size = 128\n",
        "steps_per_epoch = math.ceil(total_samples / batch_size)\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.98):\n",
        "      print(\"\\nAtingímos uma precisão maior ou igual a 95.0% então podemos parar o treino!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# Treinamento do modelo\n",
        "callbacks = myCallback()\n",
        "historicoProgesso = model.fit(\n",
        "    treina_gerador,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data = teste_gerador,\n",
        "    epochs=19,\n",
        "    verbose=2,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "xsYHyICXArnm",
        "outputId": "dcaacb97-abaf-4932-a412-5e27a2dd6598"
      },
      "outputs": [],
      "source": [
        "acc = historicoProgesso.history['accuracy']\n",
        "val_acc = historicoProgesso.history['val_accuracy']\n",
        "loss = historicoProgesso.history['loss']\n",
        "val_loss = historicoProgesso.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Precisão do conjunto de Treino')\n",
        "plt.plot(epochs, val_acc, 'b', label='Precisão do conjunto de Validação')\n",
        "plt.title('Precisão dos conjuntos de Treino e Validação')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Precisão')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lhbimI1xFmj"
      },
      "source": [
        "O gráfico gerado apresenta uma distribuição da precisão durante épocas através dos dados de treinamento e validação, é observado alguns momentos com grande diferença entre precisão de treinamento e teste em um dado ponto, isso caracteriza um overfitting. A ideia é alterar parâmetros da rede e de treino para redução desses momentos de overfitting. Mas ao final do treinamento, os dados de treino e teste estão bem próximos e ambos de maneira crescente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRqb0UMuuxuY"
      },
      "source": [
        "## Tempo de Execução com CPU e GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJLgurvsoGyg"
      },
      "source": [
        "Configurando a GPU\n",
        "\n",
        "- Selecione Runtime: No menu na parte superior do seu notebook, clique em Runtime.\n",
        "- Change runtime type: No menu que aparece, selecione Change runtime type para abrir a janela de configurações do ambiente de execução.\n",
        "- Hardware accelerator: Na janela de configurações, você verá uma opção chamada Hardware accelerator. Clique nesta opção e selecione GPU no menu dropdown.\n",
        "- Salvar: Clique em Save para salvar as configurações. O ambiente agora será reiniciado e estará configurado para usar a GPU.\n",
        "\n",
        "Se você preferir ou precisar usar a CPU em vez da GPU, você pode seguir um processo similar para configurar o notebook para usar apenas a CPU:\n",
        "\n",
        "- Selecione Runtime: No menu na parte superior do seu notebook, clique em Runtime.\n",
        "- Change runtime type: Selecione Change runtime type.\n",
        "- Hardware accelerator: Escolha None para desativar os aceleradores de hardware e usar a CPU padrão.\n",
        "- Salvar: Clique em Save. O ambiente será configurado para usar apenas a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25eA3Fg9ljs4",
        "outputId": "29175a5f-b8df-4e56-fc3c-76430233f00a"
      },
      "outputs": [],
      "source": [
        "# Identificação do dispositivo de treinamento\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(\"Treinado na GPU:\", device_name)\n",
        "else:\n",
        "    print(\"Treinado na CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAlVTNHJjbtW",
        "outputId": "29ce86ca-3eb6-4a4e-e214-69b94bb422b2"
      },
      "outputs": [],
      "source": [
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Tempo total de treinamento: {elapsed_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C6NuMUNzZBH"
      },
      "source": [
        "# Sprint 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXBVYOskzg6F"
      },
      "source": [
        "## Modelo de Segmentação de Imagens com U-Net e VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1zIEkuUzg6F"
      },
      "source": [
        "## Descrição Geral\n",
        "Este notebook implementa um modelo de segmentação de imagens utilizando a arquitetura U-Net com backbone VGG16. A segmentação é realizada em imagens e máscaras carregadas de um diretório específico. A abordagem é útil para tarefas como segmentação de áreas agrícolas em imagens de satélite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WltNUFtpzg6G"
      },
      "source": [
        "## Habilitando GPU no Google Colab\n",
        "Para habilitar a GPU no Google Colab, siga os passos abaixo:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzmMi3LNzg6G"
      },
      "source": [
        "## Importando libs necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g_gfvXkzg6G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smZu2Vq8zg6G"
      },
      "source": [
        "## Verificando se a GPU está sendo usada\n",
        "Com o código abaixo é possível verificar se a GPU está sendo usada pelo TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY3sH_ftzg6G"
      },
      "outputs": [],
      "source": [
        "# Verifica se a GPU está disponível\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"GPUs disponíveis: {gpus}\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"Nenhuma GPU disponível.\")\n",
        "\n",
        "# Verifica se o TensorFlow está utilizando a GPU\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU ativa:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"Nenhuma GPU ativa.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-3qK39Bzg6G"
      },
      "source": [
        "## Preparação de Dados\n",
        "\n",
        "A preparação de dados envolve carregar as imagens e as máscaras correspondentes, normalizando e transformando esses dados para o formato adequado para treinamento. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas, onde valores acima de 0.5 são considerados 1 (objeto de interesse) e abaixo são 0 (fundo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rtIv6aXzg6H",
        "outputId": "bf3d9254-b47e-4359-e85d-f5f08da19d40"
      },
      "outputs": [],
      "source": [
        "def load_images_and_masks(data_dir, mask_dir):\n",
        "    images = []\n",
        "    masks = []\n",
        "    file_names = sorted(os.listdir(data_dir))\n",
        "    for file in file_names:\n",
        "        if file.endswith('.png'):\n",
        "            img_path = os.path.join(data_dir, file)\n",
        "            mask_path = os.path.join(mask_dir, file)\n",
        "            try:\n",
        "                img = load_img(img_path)  # Carrega a imagem no formato padrão\n",
        "                img = img_to_array(img) / 255.0  # Normaliza a imagem para o intervalo [0, 1]\n",
        "                mask = load_img(mask_path, color_mode='grayscale')  # Carrega a máscara como grayscale\n",
        "                mask = img_to_array(mask) / 255.0  # Normaliza a máscara\n",
        "                mask = (mask > 0.5).astype(np.float32)  # Binariza a máscara\n",
        "\n",
        "                if img.shape == (1200, 1200, 3) and mask.shape == (1200, 1200, 1):\n",
        "                    images.append(img)\n",
        "                    masks.append(mask[:, :, 0])  # Garante que a máscara seja um array 2D\n",
        "                else:\n",
        "                    print(f\"Descartado por dimensões incorretas: {file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao carregar {file}: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Caminhos para os diretórios de imagens e máscaras\n",
        "data_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli/rgbs'\n",
        "mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli/masks'\n",
        "\n",
        "# Carregar imagens e máscaras\n",
        "images, masks = load_images_and_masks(data_dir, mask_dir)\n",
        "print(f\"Número de imagens: {len(images)}, Número de máscaras: {len(masks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SXCMxO3zg6H"
      },
      "source": [
        "## Função de perda personalizada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFVLbB1Uzg6H"
      },
      "source": [
        "Definimos uma função de perda personalizada que combina a perda Dice e a perda binária de entropia cruzada para otimizar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Jt0YZazg6I"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siwa-Je3zg6I"
      },
      "source": [
        "## Arquitetura do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DKPhKEczg6I"
      },
      "source": [
        "### Descrição Do Modelo Utilizado - Estrutura Geral\n",
        "\n",
        "O modelo U-Net é uma rede neural convolucional projetada para tarefas de segmentação de imagem. A arquitetura é composta por duas partes principais: o codificador (encoder) e o decodificador (decoder). A estrutura do U-Net permite capturar tanto o contexto global quanto os detalhes locais da imagem. O U-Net, proposto por Ronneberger et al. (2015), destaca-se pelo uso de conexões de skip entre o encoder e o decoder, que preservam informações de alta resolução essenciais para segmentação precisa. A extensão 3D U-Net, proposta por Çiçek et al. (2016), melhora ainda mais a capacidade do modelo em aplicações volumétricas. Ele é amplamente utilizado em diversas aplicações, incluindo a segmentação de tumores e análise de imagens médicas.\n",
        "\n",
        "Referências:\n",
        "\n",
        "Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234-241).\n",
        "\n",
        "Çiçek, Ö., et al. (2016). 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016 (pp. 424-432).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONpZL5N3zg6I"
      },
      "source": [
        "### Codificador (Encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXSXOsN0zg6I"
      },
      "source": [
        "\n",
        "O codificador é baseado na arquitetura VGG16 pré-treinada no conjunto de dados ImageNet. Ele consiste em uma série de camadas convolucionais seguidas por camadas de pooling. A função do codificador é extrair características da imagem de entrada em diferentes níveis de abstração. As camadas convolucionais e de pooling são:\n",
        "\n",
        "\n",
        "- **Block 1:**\n",
        "  - 2 camadas Conv2D com 64 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 2:**\n",
        "  - 2 camadas Conv2D com 128 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 3:**\n",
        "  - 3 camadas Conv2D com 256 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 4:**\n",
        "  - 3 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 5:**\n",
        "  - 3 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zu0bT62zg6I"
      },
      "source": [
        "### Decodificador (Decoder)\n",
        "O decodificador reconstrói a imagem de saída a partir das características extraídas pelo codificador. Utiliza operações de upsampling e camadas convolucionais para aumentar a resolução da imagem. As camadas do decodificador incluem:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQmljwwjzg6I"
      },
      "source": [
        "- **UpSampling 1:**\n",
        "  - 1 camada Conv2DTranspose com 512 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 2:**\n",
        "  - 1 camada Conv2DTranspose com 256 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 256 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 3:**\n",
        "  - 1 camada Conv2DTranspose com 128 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 128 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 4:**\n",
        "  - 1 camada Conv2DTranspose com 64 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 64 filtros e kernel 3x3, ativação ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N0j4hN6zg6J"
      },
      "source": [
        "\n",
        "### Saída (Output)\n",
        "A última camada do decodificador é uma camada Conv2D com um único filtro e função de ativação sigmoid, produzindo a máscara segmentada binária."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTsUYZ7qzg6J"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def unet_with_vgg16_backbone(input_size=(1200, 1200, 3)):\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Encoder\n",
        "    s1 = vgg16.get_layer(\"block1_conv2\").output\n",
        "    s2 = vgg16.get_layer(\"block2_conv2\").output\n",
        "    s3 = vgg16.get_layer(\"block3_conv3\").output\n",
        "    s4 = vgg16.get_layer(\"block4_conv3\").output\n",
        "    b1 = vgg16.get_layer(\"block5_conv3\").output\n",
        "\n",
        "    # Decoder\n",
        "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(b1)\n",
        "    u6 = concatenate([u6, s4])\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, s3])\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, s2])\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, s1])\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=[vgg16.input], outputs=[outputs])\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss=combined_loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOTZU9heNWEx"
      },
      "source": [
        "## Divisão dos Dados e Treinamento\n",
        "Dividimos os dados em conjuntos de treino e teste e treinamos o modelo U-Net com VGG16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYqOx2havVhl"
      },
      "outputs": [],
      "source": [
        "# Dividir os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju58IauNxKWL",
        "outputId": "d9688382-1ede-4dee-e09e-794d792597bd"
      },
      "outputs": [],
      "source": [
        "# Criar o modelo\n",
        "model = unet_with_vgg16_backbone(input_size=(1200, 1200, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L2P3jrQxO2u",
        "outputId": "729f1ec0-c3c6-4090-d099-cdf8fac2f301"
      },
      "outputs": [],
      "source": [
        "# Treinar o modelo\n",
        "results = model.fit(X_train, y_train, validation_split=0.1, batch_size=1, epochs=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7ap8iDDizg6K",
        "outputId": "f54a41d0-8138-4c0d-a304-b3f337833f63"
      },
      "outputs": [],
      "source": [
        "# Avaliar o modelo\n",
        "model.evaluate(X_test, y_test, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqEg49Dwzg6K"
      },
      "source": [
        "## Plotando métricas\n",
        "Plotamos as métricas de perda e acurácia do modelo durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "VvTxHUu1zg6K",
        "outputId": "beb233c4-4f00-48ee-e3b3-37e5296a9e9f"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(history):\n",
        "    # Plotando a perda (loss)\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Validação')\n",
        "    plt.title('Perda')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando a acurácia (accuracy)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validação')\n",
        "    plt.title('Acurácia')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzw7i8HEzg6K"
      },
      "source": [
        "## Plotando resultados\n",
        "Plotamos as imagens de entrada, as máscaras reais e as máscaras preditas pelo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGVgGUGszg6K"
      },
      "outputs": [],
      "source": [
        "# Função para plotar os resultados\n",
        "def plot_results(X, y, model, ix=None):\n",
        "    \"\"\"Plotar a imagem, a máscara real e a máscara predita.\n",
        "\n",
        "    Args:\n",
        "    - X: array de imagens de entrada.\n",
        "    - y: array de máscaras reais.\n",
        "    - model: modelo treinado U-Net.\n",
        "    - ix: índice da imagem a ser plotada. Se None, seleciona uma imagem aleatoriamente.\n",
        "    \"\"\"\n",
        "    if ix is None:\n",
        "        ix = np.random.randint(0, len(X))\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
        "    ax[0].imshow(X[ix], cmap='gray')\n",
        "    ax[0].title.set_text('Imagem Original')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(y[ix].squeeze(), cmap='gray')  # Squeeze para remover dimensões extras se houver\n",
        "    ax[1].title.set_text('Máscara Real')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    # Fazer a predição usando o modelo\n",
        "    pred = model.predict(X[ix:ix+1])\n",
        "    pred = (pred > 0.5).astype(np.float32)  # Binarizar a predição\n",
        "\n",
        "    ax[2].imshow(pred.squeeze(), cmap='gray')  # Squeeze para garantir que está em 2D\n",
        "    ax[2].title.set_text('Máscara Predita')\n",
        "    ax[2].axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "lakQgC2a2tqy",
        "outputId": "c30f314d-96b3-41b6-dea4-c6f1e21b66cf"
      },
      "outputs": [],
      "source": [
        "index = None\n",
        "plot_results(X_test, y_test, model, ix=index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjndrHTmzg6L"
      },
      "source": [
        "## Instruções para Obtenção do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VsZ5C-nzg6L"
      },
      "source": [
        "### Passo 1: Aquisição dos Dados\n",
        "Os dados utilizados neste exemplo são imagens e máscaras que podem ser carregadas a partir de um diretório local ou de uma unidade do Google Drive. Certifique-se de que as imagens e as máscaras estejam no formato PNG e que as máscaras sejam imagens em escala de cinza (grayscale).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4HbIAs5zg6M"
      },
      "source": [
        "### Passo 2: Configuração do Ambiente\n",
        "Para utilizar a GPU no Google Colab:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psxtxQvgzg6M"
      },
      "source": [
        "### Passo 3: Preparação dos Dados\n",
        "Os dados devem ser normalizados e divididos em conjuntos de treino e teste. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas (valores acima de 0.5 são considerados 1 e abaixo são 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUKCUaY3zg6M"
      },
      "source": [
        "### Passo 4: Treinamento do Modelo\n",
        "O modelo U-Net com backbone VGG16 é treinado utilizando os dados preparados. Ajuste os hiperparâmetros como número de épocas, tamanho do lote e taxa de aprendizado conforme necessário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1nO42w3zg6M"
      },
      "source": [
        "### Passo 5: Avaliação do Modelo\n",
        "Após o treinamento, o modelo é avaliado no conjunto de teste para medir seu desempenho. Métricas como a perda e a acurácia são plotadas para análise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNt1ePCQzg6N"
      },
      "source": [
        "\n",
        "### Passo 6: Refinamento do Modelo\n",
        "Se o desempenho do modelo não for satisfatório, existem as seguintes estratégias de refinamento:\n",
        "1. **Ajuste de Hiperparâmetros:** Modificar a taxa de aprendizado (learning rate ou `lr`), o número de épocas (`epochs`), o tamanho do lote (`batch_size`), etc.\n",
        "2. **Aumento de Dados (Data Augmentation):** Utilize técnicas de aumento de dados para gerar mais exemplos de treino, como rotações, zoom, translações, etc.\n",
        "3. **Arquitetura do Modelo:** Modificar a arquitetura do modelo, adicionando mais camadas ou unidades.\n",
        "4. **Regularização:** Adicionar camadas de Dropout ou ajuste a regularização L2 para evitar overfitting.\n",
        "5. **Treinamento com Mais Dados:** Adicionar mais dados de treino para melhorar a generalização do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8T-GByizg6N"
      },
      "source": [
        "## Instruções para deploy em um serviço em nuvem\n",
        "\n",
        "As seguintes instruções fornecem um guia para preparar, fazer upload e fazer o deploy do modelo de segmentação no AWS SageMaker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaGpd8P9zg6N"
      },
      "source": [
        "### 1. Preparação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QjI6_Oyzg6O"
      },
      "source": [
        "Instale a AWS CLI e configure suas credenciais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtiFYCnezg6O"
      },
      "outputs": [],
      "source": [
        "pip install awscli\n",
        "aws configure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHtSIwQkzg6O"
      },
      "source": [
        "Instale o SDK do SageMaker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ipKk17yzg6R"
      },
      "outputs": [],
      "source": [
        "pip install sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5daezevYzg6R"
      },
      "source": [
        "### 2. Preparar o notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZzMe9-Bzg6S"
      },
      "source": [
        "Certifique-se que todas as dependências do notebook estão instaladas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsn6V5wjzg6S"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas scikit-learn matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkKgAEsazg6S"
      },
      "source": [
        "### 3. Preparação do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEOPjfqwzg6T"
      },
      "source": [
        "Após o treinamento do modelo, salve-o no formato .h5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl5fobiYzg6T"
      },
      "outputs": [],
      "source": [
        "model.save('segmentation_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX_zpy44zg6U"
      },
      "source": [
        "Após isso, carregue o arquivo do modelo salvo para um bucket no S3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ORXpinzg6U"
      },
      "outputs": [],
      "source": [
        "aws s3 cp segmentation_model.h5 s3://nome-do-bucket/modelos/segmentation_model.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXeQ4fRyzg6V"
      },
      "source": [
        "### 4. Deploy no SageMaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeYiVpmgzg6V"
      },
      "source": [
        "Crie um arquivo inference.py e coloque o seguinte código que contém a lógica de carregamento do modelo e inferência:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZNpNcjNzg6V"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    model = load_model(f'{model_dir}/segmentation_model.h5')\n",
        "    return model\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    if request_content_type == 'application/json':\n",
        "        return np.array(json.loads(request_body)['instances'])\n",
        "    else:\n",
        "        raise ValueError('Esse modelo apenas suporta JSON')\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    predictions = model.predict(input_data)\n",
        "    return predictions\n",
        "\n",
        "def output_fn(prediction, response_content_type):\n",
        "    return json.dumps({'predições': prediction.tolist()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ZAFeqZzg6V"
      },
      "source": [
        "Utilize o SDK do SageMaker para criar um endpoint para o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qci8EZehzg6V"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "from sagemaker.tensorflow import TensorFlowModel\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = 'arn:aws:iam::sua-conta:permissao/sagemaker-permissao'\n",
        "\n",
        "model = TensorFlowModel(model_data='s3://nome-bucket/modelos/segmentation_model.h5',\n",
        "                        role=role,\n",
        "                        entry_point='inference.py',\n",
        "                        framework_version='2.3',\n",
        "                        sagemaker_session=sagemaker_session)\n",
        "\n",
        "predictor = model.deploy(initial_instance_count=1,\n",
        "                         instance_type='ml.m4.xlarge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKVG6OSszg6W"
      },
      "source": [
        "### 5. Teste e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1u6wII6zg6W"
      },
      "source": [
        "Envie uma requisição de teste ao endpoint para garantir que tudo esteja funcionando corretamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca3Fxjduzg6W"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import boto3\n",
        "\n",
        "runtime = boto3.client('runtime.sagemaker')\n",
        "payload = json.dumps({'instances': np.random.rand(1, 224, 224, 3).tolist()})\n",
        "\n",
        "response = runtime.invoke_endpoint(EndpointName=predictor.endpoint_name,\n",
        "                                   ContentType='application/json',\n",
        "                                   Body=payload)\n",
        "\n",
        "result = json.loads(response['Body'].read().decode())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSTgkzIjzg6W"
      },
      "source": [
        "### 6. Limpeza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VYEqM5Jzg6W"
      },
      "source": [
        "Para evitar custos desnecessários, remova os recursos quando terminar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeuYhBiszg6X"
      },
      "outputs": [],
      "source": [
        "predictor.delete_endpoint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7V1JXad0P1k"
      },
      "source": [
        "# Sprint 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnwsHGjU0P10"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XfYoT7O0P11"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# def rename_files(directory):\n",
        "#     for filename in os.listdir(directory):\n",
        "#         if filename.endswith(\".tif\") or filename.endswith(\".png\"):\n",
        "#             # Extrair o número antes do primeiro sublinhado e adicionar a extensão correta\n",
        "#             name_part = filename.split('_')[0]\n",
        "#             extension = os.path.splitext(filename)[1]\n",
        "#             new_name = name_part + extension\n",
        "\n",
        "#             # Verificar se o arquivo já está com o nome correto\n",
        "#             if filename != new_name:\n",
        "#                 old_path = os.path.join(directory, filename)\n",
        "#                 new_path = os.path.join(directory, new_name)\n",
        "#                 os.rename(old_path, new_path)\n",
        "#                 print(f\"Renamed '{filename}' to '{new_name}' in directory '{directory}'\")\n",
        "#             else:\n",
        "#                 print(f\"File '{filename}' already has the correct name in directory '{directory}'\")\n",
        "\n",
        "# def rename_directories(base_directory):\n",
        "#     for dirname in os.listdir(base_directory):\n",
        "#         dir_path = os.path.join(base_directory, dirname)\n",
        "#         if os.path.isdir(dir_path):\n",
        "#             # Extrair o número antes do primeiro sublinhado e renomear o diretório\n",
        "#             new_name = dirname.split('_')[0]\n",
        "#             new_dir_path = os.path.join(base_directory, new_name)\n",
        "\n",
        "#             # Verificar se o diretório já está com o nome correto\n",
        "#             if dirname != new_name:\n",
        "#                 os.rename(dir_path, new_dir_path)\n",
        "#                 print(f\"Renamed directory '{dirname}' to '{new_name}'\")\n",
        "#             else:\n",
        "#                 print(f\"Directory '{dirname}' already has the correct name\")\n",
        "\n",
        "# # Lista de diretórios onde estão as imagens\n",
        "# directories = [\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/marked_rgbs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_pngs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_tifs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/rgbs',\n",
        "# ]\n",
        "\n",
        "# # Diretório de imagens com subdiretórios\n",
        "# images_directory = '/content/drive/MyDrive/modulo10/data/dataset_inteli_felipe/images'\n",
        "\n",
        "# # Renomear os arquivos nos diretórios especificados\n",
        "# for directory in directories:\n",
        "#     rename_files(directory)\n",
        "\n",
        "# # Renomear os arquivos no diretório de imagens\n",
        "# rename_files(images_directory)\n",
        "\n",
        "# # Renomear os diretórios de imagens\n",
        "# rename_directories(images_directory)\n",
        "\n",
        "# print(\"Renaming completed in all directories\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pPDCT1A0P11"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from PIL import Image, UnidentifiedImageError, ImageEnhance\n",
        "\n",
        "# def rotate_image(image, angle):\n",
        "#     \"\"\"Rotate the image by a specified angle.\"\"\"\n",
        "#     return image.rotate(angle)\n",
        "\n",
        "# def adjust_brightness_contrast(image, brightness_factor, contrast_factor):\n",
        "#     \"\"\"Adjust brightness and contrast of an image.\"\"\"\n",
        "#     enhancer = ImageEnhance.Brightness(image)\n",
        "#     image = enhancer.enhance(brightness_factor)\n",
        "#     enhancer = ImageEnhance.Contrast(image)\n",
        "#     image = enhancer.enhance(contrast_factor)\n",
        "#     return image\n",
        "\n",
        "# # Função para visualizar a imagem rotacionada em 90, 180 e 270 graus\n",
        "# def visualize_specific_rotations(image_path, image_dimension):\n",
        "#     try:\n",
        "#         image = load_img(image_path, target_size=(image_dimension, image_dimension))\n",
        "#         x = img_to_array(image).astype('uint8')\n",
        "#         original_image = Image.fromarray(x)\n",
        "\n",
        "#         rotations = [90, 180, 270]\n",
        "#         fig, axes = plt.subplots(1, len(rotations), figsize=(20, 20))\n",
        "\n",
        "#         for i, angle in enumerate(rotations):\n",
        "#             rotated_image = rotate_image(original_image, angle)\n",
        "#             axes[i].imshow(rotated_image)\n",
        "#             axes[i].set_title(f\"Rotation: {angle} degrees\")\n",
        "#             axes[i].axis('off')\n",
        "\n",
        "#         plt.show()\n",
        "\n",
        "#     except UnidentifiedImageError:\n",
        "#         print(f\"Error: Cannot identify image file {image_path}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Unexpected error occurred with file {image_path}: {e}\")\n",
        "\n",
        "# # Função para visualizar a imagem com brilho e contraste ajustados\n",
        "# def visualize_brightness_contrast(image_path, image_dimension, brightness_factors, contrast_factors):\n",
        "#     try:\n",
        "#         image = load_img(image_path, target_size=(image_dimension, image_dimension))\n",
        "#         x = img_to_array(image).astype('uint8')\n",
        "#         original_image = Image.fromarray(x)\n",
        "\n",
        "#         fig, axes = plt.subplots(len(brightness_factors), len(contrast_factors), figsize=(20, 20))\n",
        "\n",
        "#         for i, brightness in enumerate(brightness_factors):\n",
        "#             for j, contrast in enumerate(contrast_factors):\n",
        "#                 adjusted_image = adjust_brightness_contrast(original_image, brightness, contrast)\n",
        "#                 axes[i, j].imshow(adjusted_image)\n",
        "#                 axes[i, j].set_title(f\"Brightness: {brightness}, Contrast: {contrast}\")\n",
        "#                 axes[i, j].axis('off')\n",
        "\n",
        "#         plt.show()\n",
        "\n",
        "#     except UnidentifiedImageError:\n",
        "#         print(f\"Error: Cannot identify image file {image_path}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Unexpected error occurred with file {image_path}: {e}\")\n",
        "\n",
        "# # Caminho para a imagem de teste\n",
        "# test_image_path = '/content/drive/MyDrive/modulo10/data/575_2019-8-14_S2L1C_21JXJ_TCI.png'\n",
        "# image_dimension = 1200\n",
        "\n",
        "# # Visualizar as rotações específicas na imagem de teste\n",
        "# visualize_specific_rotations(test_image_path, image_dimension)\n",
        "\n",
        "# # Definir fatores de brilho e contraste\n",
        "# brightness_factors = [1.0, 1.5]  # Exemplos de fatores de brilho\n",
        "# contrast_factors = [1.0, 1.5]    # Exemplos de fatores de contraste\n",
        "\n",
        "# # Visualizar os ajustes de brilho e contraste na imagem de teste\n",
        "# visualize_brightness_contrast(test_image_path, image_dimension, brightness_factors, contrast_factors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO41GGEb0P11"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from PIL import Image, UnidentifiedImageError, ImageEnhance\n",
        "# import numpy as np\n",
        "\n",
        "# def rotate_image(image, angle):\n",
        "#     \"\"\"Rotate the image by a specified angle.\"\"\"\n",
        "#     return image.rotate(angle)\n",
        "\n",
        "# def adjust_brightness_contrast(image, brightness_factor, contrast_factor):\n",
        "#     \"\"\"Adjust brightness and contrast of an image.\"\"\"\n",
        "#     enhancer = ImageEnhance.Brightness(image)\n",
        "#     image = enhancer.enhance(brightness_factor)\n",
        "#     enhancer = ImageEnhance.Contrast(image)\n",
        "#     image = enhancer.enhance(contrast_factor)\n",
        "#     return image\n",
        "\n",
        "# def data_augmentation(directory, image_dimension):\n",
        "#     images_generated = 0\n",
        "#     rotations = [90, 180, 270]\n",
        "#     brightness_factors = [1, 1.5]  # Factors for decreasing and increasing brightness\n",
        "#     contrast_factors = [1, 1.5]    # Factors for decreasing and increasing contrast\n",
        "\n",
        "#     for filename in os.listdir(directory):\n",
        "#         if filename.endswith(\".tif\") or filename.endswith(\".png\"):\n",
        "#             image_path = os.path.join(directory, filename)\n",
        "#             print(f\"Processing file: {filename}\")\n",
        "\n",
        "#             try:\n",
        "#                 image = load_img(image_path, target_size=(image_dimension, image_dimension))\n",
        "#                 x = img_to_array(image)\n",
        "\n",
        "#                 name, ext = os.path.splitext(filename)\n",
        "\n",
        "#                 # Apply rotations\n",
        "#                 for angle in rotations:\n",
        "#                     rotated_image = rotate_image(Image.fromarray(x.astype('uint8')), angle)\n",
        "#                     new_name = f\"{name}_rot{angle}{ext}\"\n",
        "#                     save_path = os.path.join(directory, new_name)\n",
        "#                     rotated_image.save(save_path)\n",
        "#                     images_generated += 1\n",
        "\n",
        "#                     # Apply brightness and contrast adjustments\n",
        "#                     for brightness in brightness_factors:\n",
        "#                         for contrast in contrast_factors:\n",
        "#                             adjusted_image = adjust_brightness_contrast(rotated_image, brightness, contrast)\n",
        "#                             new_name = f\"{name}_rot{angle}_bright{brightness}_cont{contrast}{ext}\"\n",
        "#                             save_path = os.path.join(directory, new_name)\n",
        "#                             adjusted_image.save(save_path)\n",
        "#                             images_generated += 1\n",
        "\n",
        "#             except UnidentifiedImageError:\n",
        "#                 print(f\"Error: Cannot identify image file {image_path}\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Unexpected error occurred with file {filename}: {e}\")\n",
        "\n",
        "#     return images_generated\n",
        "\n",
        "# # Lista de diretórios onde estão as imagens\n",
        "# directories = [\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_pngs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_tifs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/rgbs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/masks'  # Incluindo o diretório de máscaras\n",
        "# ]\n",
        "\n",
        "# image_dimension = 1200\n",
        "\n",
        "# # Aplicar data augmentation em todos os diretórios especificados e contar imagens geradas\n",
        "# total_images_generated = 0\n",
        "# for directory in directories:\n",
        "#     images_generated = data_augmentation(directory, image_dimension)\n",
        "#     total_images_generated += images_generated\n",
        "#     print(f\"Number of images generated in {directory}: {images_generated}\")\n",
        "\n",
        "# print(f\"Total number of images generated in all directories: {total_images_generated}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5wHF3tx0P11"
      },
      "outputs": [],
      "source": [
        "# image_path = '/content/drive/MyDrive/modulo10/data/dataset_inteli_felipe/tci_tifs/1062_2020-8-8_S2L1C_21JYK_TCI.tif'\n",
        "\n",
        "\n",
        "# with rasterio.open(image_path) as src:\n",
        "#     image_data = src.read(1)\n",
        "\n",
        "# plt.imshow(image_data)\n",
        "# plt.axis('off')\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMCN6_070P12"
      },
      "outputs": [],
      "source": [
        "# image_path = '/content/drive/MyDrive/modulo10/data/dataset_inteli_felipe/images/1062_2020-8-8_S2L1C_21JYK/b11.tif'\n",
        "\n",
        "# with rasterio.open(image_path) as src:\n",
        "#     image_data = src.read(1)\n",
        "\n",
        "# plt.imshow(image_data)\n",
        "# plt.axis('off')\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW8oLZ9w0P12"
      },
      "source": [
        "## Crop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-KtQ42z0P12"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from PIL import Image, UnidentifiedImageError, ImageEnhance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dmbpwEZ0P12"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "# import os\n",
        "# import uuid\n",
        "\n",
        "# # Diretórios contendo as imagens originais\n",
        "# input_image_dirs = [\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/rgbs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_tifs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_pngs',\n",
        "# ]\n",
        "# # Diretório contendo as máscaras\n",
        "# input_mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/masks'\n",
        "\n",
        "# # Diretórios onde as imagens e máscaras cortadas serão salvas\n",
        "# output_image_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/newTargetImages2'\n",
        "# output_mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/newTargetMasks2'\n",
        "\n",
        "# os.makedirs(output_image_dir, exist_ok=True)\n",
        "# os.makedirs(output_mask_dir, exist_ok=True)\n",
        "\n",
        "# # Dimensões dos cortes\n",
        "# crop_width, crop_height = 120, 120\n",
        "\n",
        "# # Cortes por imagem\n",
        "# max_cuts = 5\n",
        "\n",
        "# # Função para cortar a imagem e sua máscara em pedaços menores\n",
        "# def crop_image_and_mask(image_path, mask_path, output_image_dir, output_mask_dir, crop_width, crop_height, max_cuts):\n",
        "#     try:\n",
        "#         img = Image.open(image_path)\n",
        "#         mask = Image.open(mask_path)\n",
        "#         img_width, img_height = img.size\n",
        "\n",
        "#         # Gera um nome aleatório para a imagem\n",
        "#         base_name = str(uuid.uuid4())[:8]\n",
        "#         ext = os.path.splitext(image_path)[1]\n",
        "\n",
        "#         cut_count = 0\n",
        "#         for i in range(0, img_width, crop_width):\n",
        "#             for j in range(0, img_height, crop_height):\n",
        "#                 if cut_count >= max_cuts:\n",
        "#                     return\n",
        "#                 box = (i, j, i + crop_width, j + crop_height)\n",
        "#                 cropped_img = img.crop(box)\n",
        "#                 cropped_mask = mask.crop(box)\n",
        "#                 cropped_img_path = os.path.join(output_image_dir, f\"{base_name}_{cut_count+1}{ext}\")\n",
        "#                 cropped_mask_path = os.path.join(output_mask_dir, f\"{base_name}_{cut_count+1}{ext}\")\n",
        "#                 cropped_img.save(cropped_img_path)\n",
        "#                 cropped_mask.save(cropped_mask_path)\n",
        "#                 cut_count += 1\n",
        "#     except Exception as e:\n",
        "#         print(f\"Erro ao processar {image_path}: {e}\")\n",
        "\n",
        "# # Percorrer todos os diretórios de entrada\n",
        "# for input_image_dir in input_image_dirs:\n",
        "#     for filename in os.listdir(input_image_dir):\n",
        "#         if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "#             image_path = os.path.join(input_image_dir, filename)\n",
        "#             mask_path = os.path.join(input_mask_dir, filename.replace('.tif', '.png'))  # Assumindo que as máscaras têm o mesmo nome que as imagens\n",
        "#             if os.path.exists(mask_path):  # Garantindo que a máscara correspondente existe\n",
        "#                 print(f\"Processando {image_path} e {mask_path}\")\n",
        "#                 crop_image_and_mask(image_path, mask_path, output_image_dir, output_mask_dir, crop_width, crop_height, max_cuts)\n",
        "#             else:\n",
        "#                 print(f\"Máscara não encontrada para {image_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAEdxmni0P12"
      },
      "source": [
        "## Modelo de Segmentação de Imagens com U-Net e VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glfiTMEk0P12"
      },
      "source": [
        "## Descrição Geral\n",
        "Este notebook implementa um modelo de segmentação de imagens utilizando a arquitetura U-Net com backbone VGG16. A segmentação é realizada em imagens e máscaras carregadas de um diretório específico. A abordagem é útil para tarefas como segmentação de áreas agrícolas em imagens de satélite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20FlSunq0P12"
      },
      "source": [
        "## Habilitando GPU no Google Colab\n",
        "Para habilitar a GPU no Google Colab, siga os passos abaixo:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsC8zCV0P12"
      },
      "source": [
        "## Importando libs necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4augWU-f0P13"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import gdown\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Conv2DTranspose\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOMXuAW0P13"
      },
      "source": [
        "## Verificando se a GPU está sendo usada\n",
        "Com o código abaixo é possível verificar se a GPU está sendo usada pelo TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gWCKmXj0P13",
        "outputId": "b356f4c7-e66f-4e8b-aaf3-83c164b82aff"
      },
      "outputs": [],
      "source": [
        "# Verificação da GPU\n",
        "def check_gpu():\n",
        "    \"\"\"Verifica se a GPU está disponível e ativa.\"\"\"\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(f\"GPUs disponíveis: {gpus}\")\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    else:\n",
        "        print(\"Nenhuma GPU disponível.\")\n",
        "\n",
        "    if tf.test.gpu_device_name():\n",
        "        print('GPU ativa:', tf.test.gpu_device_name())\n",
        "    else:\n",
        "        print(\"Nenhuma GPU ativa.\")\n",
        "\n",
        "check_gpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKc4NMa10P13"
      },
      "source": [
        "## Preparação de Dados\n",
        "\n",
        "A preparação de dados envolve baixar e carregar tanto as imagens quanto as máscaras correspondentes, normalizando e transformando esses dados para o formato adequado para treinamento. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas, onde valores acima de 0.5 são considerados 1 (objeto de interesse) e abaixo são 0 (fundo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_o10H520P13",
        "outputId": "493feb09-0391-45f6-ca93-080cef22638d"
      },
      "outputs": [],
      "source": [
        "def load_images_and_masks_in_batches(data_dir, mask_dir, batch_size=51):\n",
        "    \"\"\"Carrega imagens e máscaras em batches.\"\"\"\n",
        "    file_names = sorted(os.listdir(data_dir))\n",
        "    images, masks = [], []\n",
        "    count = 0\n",
        "\n",
        "    for file in file_names:\n",
        "        if file.endswith('.png') or file.endswith('.tif'):\n",
        "            img_path = os.path.join(data_dir, file)\n",
        "            mask_path = os.path.join(mask_dir, file)\n",
        "            try:\n",
        "                img = load_img(img_path)\n",
        "                img = img_to_array(img) / 255.0\n",
        "                mask = load_img(mask_path, color_mode='grayscale')\n",
        "                mask = img_to_array(mask) / 255.0\n",
        "                mask = (mask > 0.5).astype(np.float32)\n",
        "\n",
        "                if img.shape == (120, 120, 3) and mask.shape == (120, 120, 1):\n",
        "                    images.append(img)\n",
        "                    masks.append(mask[:, :, 0])\n",
        "                    count += 1\n",
        "                else:\n",
        "                    print(f\"Descartado por dimensões incorretas: {file}\")\n",
        "\n",
        "                if count == batch_size:\n",
        "                    yield np.array(images), np.array(masks)\n",
        "                    images, masks = [], []\n",
        "                    count = 0\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao carregar {file}: {e}\")\n",
        "\n",
        "    if images and masks:\n",
        "        yield np.array(images), np.array(masks)\n",
        "\n",
        "# Caminhos para os diretórios de imagens e máscaras\n",
        "data_dir = './data/newTargetImages2'\n",
        "mask_dir = './data/newTargetMasks2'\n",
        "\n",
        "# Processar imagens e máscaras em batches\n",
        "batch_size = 51\n",
        "all_images, all_masks = [], []\n",
        "\n",
        "for batch_images, batch_masks in load_images_and_masks_in_batches(data_dir, mask_dir, batch_size):\n",
        "    all_images.extend(batch_images)\n",
        "    all_masks.extend(batch_masks)\n",
        "\n",
        "# Converter listas para arrays numpy\n",
        "all_images = np.array(all_images)\n",
        "all_masks = np.array(all_masks)\n",
        "\n",
        "# Dividir os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_images, all_masks, test_size=0.1, random_state=42)\n",
        "print(f\"Tamanho do conjunto de treino: {len(X_train)} imagens\")\n",
        "print(f\"Tamanho do conjunto de teste: {len(X_test)} imagens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXAhtwtF0P14"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# def load_images_and_masks_in_batches(data_dir, mask_dir, batch_size=51):\n",
        "#     file_names = sorted(os.listdir(data_dir))\n",
        "#     images = []\n",
        "#     masks = []\n",
        "#     count = 0\n",
        "\n",
        "#     for file in file_names:\n",
        "#         if file.endswith('.png'):\n",
        "#             img_path = os.path.join(data_dir, file)\n",
        "#             mask_path = os.path.join(mask_dir, file)\n",
        "#             try:\n",
        "#                 img = load_img(img_path)  # Carrega a imagem no formato padrão\n",
        "#                 img = img_to_array(img) / 255.0  # Normaliza a imagem para o intervalo [0, 1]\n",
        "#                 mask = load_img(mask_path, color_mode='grayscale')  # Carrega a máscara como grayscale\n",
        "#                 mask = img_to_array(mask) / 255.0  # Normaliza a máscara\n",
        "#                 mask = (mask > 0.5).astype(np.float32)  # Binariza a máscara\n",
        "\n",
        "#                 if img.shape == (120, 120, 3) and mask.shape == (120, 120, 1):\n",
        "#                     images.append(img)\n",
        "#                     masks.append(mask[:, :, 0])  # Garante que a máscara seja um array 2D\n",
        "#                     count += 1\n",
        "#                 else:\n",
        "#                     print(f\"Descartado por dimensões incorretas: {file}\")\n",
        "\n",
        "#                 if count == batch_size:\n",
        "#                     yield np.array(images), np.array(masks)\n",
        "#                     images, masks = [], []\n",
        "#                     count = 0\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Erro ao carregar {file}: {e}\")\n",
        "\n",
        "#     if images and masks:\n",
        "#         yield np.array(images), np.array(masks)\n",
        "\n",
        "# # Caminhos para os diretórios de imagens e máscaras\n",
        "# data_dir = './data/newTargetImages2'\n",
        "# mask_dir = './data/newTargetMasks2'\n",
        "\n",
        "# # Processar imagens e máscaras em batches\n",
        "# batch_size = 51\n",
        "# all_images = []\n",
        "# all_masks = []\n",
        "\n",
        "# for batch_images, batch_masks in load_images_and_masks_in_batches(data_dir, mask_dir, batch_size):\n",
        "#     all_images.extend(batch_images)\n",
        "#     all_masks.extend(batch_masks)\n",
        "\n",
        "# # Converter listas para arrays numpy\n",
        "# all_images = np.array(all_images)\n",
        "# all_masks = np.array(all_masks)\n",
        "\n",
        "# # Dividir os dados em treino e teste\n",
        "# X_train, X_test, y_train, y_test = train_test_split(all_images, all_masks, test_size=0.1, random_state=42)\n",
        "\n",
        "# print(f\"Tamanho do conjunto de treino: {len(X_train)} imagens\")\n",
        "# print(f\"Tamanho do conjunto de teste: {len(X_test)} imagens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIIvkNXB0P14"
      },
      "source": [
        "## Funções de perda e métricas personalizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehOL08k80P15"
      },
      "source": [
        "Definimos uma função de perda personalizada que combina a perda Dice e a perda binária de entropia cruzada para otimizar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAyRZFKe0P15"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def iou_metric(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    return intersection / (union + tf.keras.backend.epsilon())\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n",
        "    return true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    y_true = tf.round(y_true)\n",
        "    return tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32)) / tf.reduce_sum(tf.cast(tf.size(y_true), tf.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFscBGCy0P15"
      },
      "source": [
        "## Arquitetura do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av1k_pTf0P15"
      },
      "source": [
        "### Descrição Do Modelo Utilizado - Estrutura Geral\n",
        "\n",
        "O modelo U-Net é uma rede neural convolucional projetada para tarefas de segmentação de imagem. A arquitetura é composta por duas partes principais: o codificador (encoder) e o decodificador (decoder). A estrutura do U-Net permite capturar tanto o contexto global quanto os detalhes locais da imagem. O U-Net, proposto por Ronneberger et al. (2015), destaca-se pelo uso de conexões de skip entre o encoder e o decoder, que preservam informações de alta resolução essenciais para segmentação precisa. A extensão 3D U-Net, proposta por Çiçek et al. (2016), melhora ainda mais a capacidade do modelo em aplicações volumétricas. Ele é amplamente utilizado em diversas aplicações, incluindo a segmentação de tumores e análise de imagens médicas.\n",
        "\n",
        "Referências:\n",
        "\n",
        "Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234-241).\n",
        "\n",
        "Çiçek, Ö., et al. (2016). 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016 (pp. 424-432).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaQntki_0P15"
      },
      "source": [
        "### Codificador (Encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVkaPj2g0P15"
      },
      "source": [
        "\n",
        "O codificador é baseado na arquitetura VGG16 pré-treinada no conjunto de dados ImageNet. Ele consiste em uma série de camadas convolucionais seguidas por camadas de pooling. A função do codificador é extrair características da imagem de entrada em diferentes níveis de abstração. As camadas convolucionais e de pooling são:\n",
        "\n",
        "\n",
        "- **Block 1:**\n",
        "  - 2 camadas Conv2D com 64 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 2:**\n",
        "  - 2 camadas Conv2D com 128 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 3:**\n",
        "  - 3 camadas Conv2D com 256 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 4:**\n",
        "  - 3 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 5:**\n",
        "  - 3 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xxgsbqo0P15"
      },
      "source": [
        "### Decodificador (Decoder)\n",
        "O decodificador reconstrói a imagem de saída a partir das características extraídas pelo codificador. Utiliza operações de upsampling e camadas convolucionais para aumentar a resolução da imagem. As camadas do decodificador incluem:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exbw1Xu30P15"
      },
      "source": [
        "- **UpSampling 1:**\n",
        "  - 1 camada Conv2DTranspose com 512 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 2:**\n",
        "  - 1 camada Conv2DTranspose com 256 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 256 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 3:**\n",
        "  - 1 camada Conv2DTranspose com 128 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 128 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 4:**\n",
        "  - 1 camada Conv2DTranspose com 64 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 64 filtros e kernel 3x3, ativação ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzabQhH0P15"
      },
      "source": [
        "\n",
        "### Saída (Output)\n",
        "A última camada do decodificador é uma camada Conv2D com um único filtro e função de ativação sigmoid, produzindo a máscara segmentada binária."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYSm6knc0P15"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, concatenate, Resizing, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def unet_with_vgg16_backbone(input_size=(120, 120, 3), dropout_rate=0.5, loss_function='binary_crossentropy'):\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "\n",
        "    # Encoder\n",
        "    s1 = vgg16.get_layer(\"block1_conv2\").output\n",
        "    s2 = vgg16.get_layer(\"block2_conv2\").output\n",
        "    s3 = vgg16.get_layer(\"block3_conv3\").output\n",
        "    s4 = vgg16.get_layer(\"block4_conv3\").output\n",
        "    b1 = vgg16.get_layer(\"block5_conv3\").output\n",
        "\n",
        "    # Decoder\n",
        "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(b1)\n",
        "    u6 = Resizing(s4.shape[1], s4.shape[2])(u6)\n",
        "    u6 = concatenate([u6, s4])\n",
        "    u6 = Dropout(dropout_rate)(u6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = Resizing(s3.shape[1], s3.shape[2])(u7)\n",
        "    u7 = concatenate([u7, s3])\n",
        "    u7 = Dropout(dropout_rate)(u7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = Resizing(s2.shape[1], s2.shape[2])(u8)\n",
        "    u8 = concatenate([u8, s2])\n",
        "    u8 = Dropout(dropout_rate)(u8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = Resizing(s1.shape[1], s1.shape[2])(u9)\n",
        "    u9 = concatenate([u9, s1])\n",
        "    u9 = Dropout(dropout_rate)(u9)\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=vgg16.input, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK-PtPdk0DMr"
      },
      "source": [
        "## Avaliando o melhor valor de Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h5EUASCH0P16",
        "outputId": "5687cf1e-04b6-4076-abad-6ed2cb857083"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dropouts = np.concatenate((np.arange(0.01, 0.1, 0.01), np.arange(0.1, 0.6, 0.1)))\n",
        "val_losses = []\n",
        "dropout_values = []\n",
        "\n",
        "for dropout in dropouts:\n",
        "    print(f\"Testing model with {dropout*100:.1f}% dropout rate...\")\n",
        "    model = unet_with_vgg16_backbone(dropout_rate=dropout, loss_function=combined_loss)\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, verbose=1)\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "    val_losses.append(val_loss)\n",
        "    dropout_values.append(dropout)\n",
        "    print(f\"Validation loss for {dropout*100:.1f}% dropout: {val_loss:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(dropout_values, val_losses, marker='o', linestyle='-')\n",
        "plt.title('Validation Loss vs. Dropout Rate')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.xticks(dropout_values, labels=[f\"{d*100:.1f}%\" for d in dropout_values])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ou1ICdC0P16"
      },
      "source": [
        "## Divisão dos Dados e Treinamento\n",
        "Dividimos os dados em conjuntos de treino e teste e treinamos o modelo U-Net com VGG16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ct4ve-S0P16",
        "outputId": "f0485f93-4390-4803-be63-09456094b613"
      },
      "outputs": [],
      "source": [
        "# Criar o modelo\n",
        "model = unet_with_vgg16_backbone(input_size=(120, 120, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r9GQHPBehDG"
      },
      "outputs": [],
      "source": [
        "# Callback para salvar o modelo com a melhor acurácia de validação\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    'best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAJ0TX2BekIF"
      },
      "outputs": [],
      "source": [
        "# Callback para early stopping\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj18tSgj0P16",
        "outputId": "bc2edd39-c911-40cc-8d37-2030efcb6655"
      },
      "outputs": [],
      "source": [
        "# Treinar o modelo com os callbacks\n",
        "results = model.fit(\n",
        "    X_train, y_train, validation_split=0.1, epochs=40, batch_size=1,\n",
        "    callbacks=[early_stopping_callback, model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sybq0Ygo0P17"
      },
      "source": [
        "## Avaliação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttrz1c9v0P17"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Avalia o modelo treinado no conjunto de teste.\"\"\"\n",
        "    model.evaluate(X_test, y_test, batch_size=1)\n",
        "    model.save('segmentation_model.h5')\n",
        "\n",
        "evaluate_model(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuh9Q7Zk0P17"
      },
      "source": [
        "## Plotando métricas\n",
        "Plotamos as métricas de perda e acurácia do modelo durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiiwnaVn0P17"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(history):\n",
        "    \"\"\"Plota as métricas de perda e acurácia do modelo durante o treinamento.\"\"\"\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Plotando a perda (loss)\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(history.history['loss'], label='Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Validação')\n",
        "    plt.title('Perda')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando a acurácia (accuracy)\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validação')\n",
        "    plt.title('Acurácia')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando a métrica IOU\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(history.history['iou_metric'], label='Treino')\n",
        "    plt.plot(history.history['val_iou_metric'], label='Validação')\n",
        "    plt.title('IOU')\n",
        "    plt.ylabel('IOU')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando a precisão (precision)\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(history.history['precision'], label='Treino')\n",
        "    plt.plot(history.history['val_precision'], label='Validação')\n",
        "    plt.title('Precisão')\n",
        "    plt.ylabel('Precisão')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando o recall\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(history.history['recall'], label='Treino')\n",
        "    plt.plot(history.history['val_recall'], label='Validação')\n",
        "    plt.title('Recall')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando a pontuação F1\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(history.history['f1_score'], label='Treino')\n",
        "    plt.plot(history.history['val_f1_score'], label='Validação')\n",
        "    plt.title('Pontuação F1')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RazbgXrT0P17"
      },
      "source": [
        "## Função de Pós-Processamento para Máscaras de Segmentação\n",
        "\n",
        "Realização do pós-processamento das máscaras binárias geradas. O objetivo é suavizar, preencher quando possível e arredondar as bordas da máscara, melhorando a aparência visual das previsões e aumentando a precisão prática da segmentação.\n",
        "\n",
        "### Parâmetros\n",
        "\n",
        "- `mask (np.array)`: A máscara binária original predita pelo modelo.\n",
        "- `kernel_size (int)`: O tamanho do kernel utilizado para as operações morfológicas. Um kernel maior intensifica os efeitos de erosão e dilatação.\n",
        "\n",
        "### Processo\n",
        "\n",
        "1. **Erosão**: A erosão é aplicada primeiro para eliminar pequenos ruídos e separar objetos que estão levemente conectados. Isso ajuda a clarificar a máscara e reduzir artefatos indesejados.\n",
        "\n",
        "2. **Dilatação**: Após a erosão, a dilatação é aplicada para restaurar o tamanho dos objetos que foram erodidos, ao mesmo tempo que mantém as desconexões introduzidas pela erosão. Isso ajuda a preservar a integridade estrutural dos objetos na máscara, enquanto ainda suaviza as bordas.\n",
        "\n",
        "<!-- ### Exemplo de Código\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def post_process_mask(mask, kernel_size=3):\n",
        "    \"\"\"\n",
        "    Aplica erosão e dilatação para suavizar as bordas da máscara.\n",
        "    \"\"\"\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    erosion = cv2.erode(mask, kernel, iterations=1)\n",
        "    dilation = cv2.dilate(erosion, kernel, iterations=1)\n",
        "    return dilation -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sznWQwb00P18"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def post_process_mask(mask, kernel_size=3):\n",
        "    \"\"\"Aplica erosão seguida de dilatação para suavizar as bordas da máscara.\"\"\"\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    erosion = cv2.erode(mask, kernel, iterations=1)\n",
        "    dilation = cv2.dilate(erosion, kernel, iterations=1)\n",
        "    return dilation\n",
        "\n",
        "def plot_results(X, y, model, num_images=4):\n",
        "    \"\"\"Plota a imagem, a máscara real, a máscara predita e a máscara pós-processada para um número especificado de imagens.\"\"\"\n",
        "    for i in range(num_images):\n",
        "        ix = np.random.randint(0, len(X))\n",
        "        fig, ax = plt.subplots(1, 4, figsize=(27, 10))\n",
        "\n",
        "        # Plotando a imagem original\n",
        "        ax[0].imshow(X[ix], cmap='gray')\n",
        "        ax[0].title.set_text('Imagem Original')\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        # Plotando a máscara real\n",
        "        ax[1].imshow(y[ix].squeeze(), cmap='gray')\n",
        "        ax[1].title.set_text('Máscara Real')\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        # Fazendo a previsão da máscara e plotando\n",
        "        pred = model.predict(X[ix:ix+1])\n",
        "        pred = (pred > 0.5).astype(np.float32)\n",
        "        ax[2].imshow(pred.squeeze(), cmap='gray')\n",
        "        ax[2].title.set_text('Máscara Predita')\n",
        "        ax[2].axis('off')\n",
        "\n",
        "        # Aplicando pós-processamento na máscara predita e plotando\n",
        "        pred_processed = post_process_mask(pred.squeeze())\n",
        "        ax[3].imshow(pred_processed, cmap='gray')\n",
        "        ax[3].title.set_text('Máscara Pós-processada')\n",
        "        ax[3].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "plot_results(X_test, y_test, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwwIaAxn0P18"
      },
      "source": [
        "## Instruções para Obtenção do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBbm_AzD0P19"
      },
      "source": [
        "### Passo 1: Aquisição dos Dados\n",
        "Os dados utilizados neste exemplo são imagens e máscaras que podem ser carregadas a partir de um diretório local ou de uma unidade do Google Drive. Certifique-se de que as imagens e as máscaras estejam no formato PNG e que as máscaras sejam imagens em escala de cinza (grayscale).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkR0kowc0P19"
      },
      "source": [
        "### Passo 2: Configuração do Ambiente\n",
        "Para utilizar a GPU no Google Colab:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91GT_1-m0P19"
      },
      "source": [
        "### Passo 3: Preparação dos Dados\n",
        "Os dados devem ser normalizados e divididos em conjuntos de treino e teste. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas (valores acima de 0.5 são considerados 1 e abaixo são 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzzd7FBZ0P19"
      },
      "source": [
        "### Passo 4: Treinamento do Modelo\n",
        "O modelo U-Net com backbone VGG16 é treinado utilizando os dados preparados. Ajuste os hiperparâmetros como número de épocas, tamanho do lote e taxa de aprendizado conforme necessário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkGAEHEk0P1-"
      },
      "source": [
        "### Passo 5: Avaliação do Modelo\n",
        "Após o treinamento, o modelo é avaliado no conjunto de teste para medir seu desempenho. Métricas como a perda e a acurácia são plotadas para análise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JutaG__0P1-"
      },
      "source": [
        "\n",
        "### Passo 6: Refinamento do Modelo\n",
        "Se o desempenho do modelo não for satisfatório, existem as seguintes estratégias de refinamento:\n",
        "1. **Ajuste de Hiperparâmetros:** Modificar a taxa de aprendizado (learning rate ou `lr`), o número de épocas (`epochs`), o tamanho do lote (`batch_size`), etc.\n",
        "2. **Aumento de Dados (Data Augmentation):** Utilize técnicas de aumento de dados para gerar mais exemplos de treino, como rotações, zoom, translações, etc.\n",
        "3. **Arquitetura do Modelo:** Modificar a arquitetura do modelo, adicionando mais camadas ou unidades.\n",
        "4. **Regularização:** Adicionar camadas de Dropout ou ajuste a regularização L2 para evitar overfitting.\n",
        "5. **Treinamento com Mais Dados:** Adicionar mais dados de treino para melhorar a generalização do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfcMX6IW0P1-"
      },
      "source": [
        "## Instruções para deploy em um serviço em nuvem\n",
        "\n",
        "As seguintes instruções fornecem um guia para preparar, fazer upload e fazer o deploy do modelo de segmentação no AWS SageMaker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sHz9ryS0P1-"
      },
      "source": [
        "### 1. Preparação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdRZmxO-0P1-"
      },
      "source": [
        "Instale a AWS CLI e configure suas credenciais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1MjLVtX0P1-"
      },
      "outputs": [],
      "source": [
        "pip install awscli\n",
        "aws configure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1wBIjaD0P1-"
      },
      "source": [
        "Instale o SDK do SageMaker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ldNs00S0P1_"
      },
      "outputs": [],
      "source": [
        "pip install sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WauSOJ5p0P1_"
      },
      "source": [
        "### 2. Preparar o notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFd-Hui00P1_"
      },
      "source": [
        "Certifique-se que todas as dependências do notebook estão instaladas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqle__cP0P1_"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas scikit-learn matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDPAWX_30P1_"
      },
      "source": [
        "### 3. Preparação do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3aFUoz0P1_"
      },
      "source": [
        "Após o treinamento do modelo, salve-o no formato .h5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRRZiMu00P1_"
      },
      "outputs": [],
      "source": [
        "model.save('segmentation_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL4p-4xV0P1_"
      },
      "source": [
        "Após isso, carregue o arquivo do modelo salvo para um bucket no S3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq5llWhu0P2A"
      },
      "outputs": [],
      "source": [
        "aws s3 cp segmentation_model.h5 s3://nome-do-bucket/modelos/segmentation_model.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmbE5wbq0P2A"
      },
      "source": [
        "### 4. Deploy no SageMaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdtcRJZY0P2A"
      },
      "source": [
        "Crie um arquivo inference.py e coloque o seguinte código que contém a lógica de carregamento do modelo e inferência:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsMVYjLN0P2A"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    model = load_model(f'{model_dir}/segmentation_model.h5')\n",
        "    return model\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    if request_content_type == 'application/json':\n",
        "        return np.array(json.loads(request_body)['instances'])\n",
        "    else:\n",
        "        raise ValueError('Esse modelo apenas suporta JSON')\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    predictions = model.predict(input_data)\n",
        "    return predictions\n",
        "\n",
        "def output_fn(prediction, response_content_type):\n",
        "    return json.dumps({'predições': prediction.tolist()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM7IYusX0P2A"
      },
      "source": [
        "Utilize o SDK do SageMaker para criar um endpoint para o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea5QqNe10P2A"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "from sagemaker.tensorflow import TensorFlowModel\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = 'arn:aws:iam::sua-conta:permissao/sagemaker-permissao'\n",
        "\n",
        "model = TensorFlowModel(model_data='s3://nome-bucket/modelos/segmentation_model.h5',\n",
        "                        role=role,\n",
        "                        entry_point='inference.py',\n",
        "                        framework_version='2.3',\n",
        "                        sagemaker_session=sagemaker_session)\n",
        "\n",
        "predictor = model.deploy(initial_instance_count=1,\n",
        "                         instance_type='ml.m4.xlarge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MObsOIcG0P2A"
      },
      "source": [
        "### 5. Teste e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1v5oEEu0P2A"
      },
      "source": [
        "Envie uma requisição de teste ao endpoint para garantir que tudo esteja funcionando corretamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1296rS70P2B"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import boto3\n",
        "\n",
        "runtime = boto3.client('runtime.sagemaker')\n",
        "payload = json.dumps({'instances': np.random.rand(1, 224, 224, 3).tolist()})\n",
        "\n",
        "response = runtime.invoke_endpoint(EndpointName=predictor.endpoint_name,\n",
        "                                   ContentType='application/json',\n",
        "                                   Body=payload)\n",
        "\n",
        "result = json.loads(response['Body'].read().decode())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8qS1nhd0P2B"
      },
      "source": [
        "### 6. Limpeza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiMUDJO50P2B"
      },
      "source": [
        "Para evitar custos desnecessários, remova os recursos quando terminar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm_HrPyB0P2B"
      },
      "outputs": [],
      "source": [
        "predictor.delete_endpoint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSDKc0p5vcEh"
      },
      "source": [
        "# Sprint 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0kvPP3jZlKX"
      },
      "source": [
        "## Pré-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0GgLVB6fI_T"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-fW_j4ymwvm"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# def rename_files(directory):\n",
        "#     for filename in os.listdir(directory):\n",
        "#         if filename.endswith(\".tif\") or filename.endswith(\".png\"):\n",
        "#             # Extrair o número antes do primeiro sublinhado e adicionar a extensão correta\n",
        "#             name_part = filename.split('_')[0]\n",
        "#             extension = os.path.splitext(filename)[1]\n",
        "#             new_name = name_part + extension\n",
        "\n",
        "#             # Verificar se o arquivo já está com o nome correto\n",
        "#             if filename != new_name:\n",
        "#                 old_path = os.path.join(directory, filename)\n",
        "#                 new_path = os.path.join(directory, new_name)\n",
        "#                 os.rename(old_path, new_path)\n",
        "#                 print(f\"Renamed '{filename}' to '{new_name}' in directory '{directory}'\")\n",
        "#             else:\n",
        "#                 print(f\"File '{filename}' already has the correct name in directory '{directory}'\")\n",
        "\n",
        "# def rename_directories(base_directory):\n",
        "#     for dirname in os.listdir(base_directory):\n",
        "#         dir_path = os.path.join(base_directory, dirname)\n",
        "#         if os.path.isdir(dir_path):\n",
        "#             # Extrair o número antes do primeiro sublinhado e renomear o diretório\n",
        "#             new_name = dirname.split('_')[0]\n",
        "#             new_dir_path = os.path.join(base_directory, new_name)\n",
        "\n",
        "#             # Verificar se o diretório já está com o nome correto\n",
        "#             if dirname != new_name:\n",
        "#                 os.rename(dir_path, new_dir_path)\n",
        "#                 print(f\"Renamed directory '{dirname}' to '{new_name}'\")\n",
        "#             else:\n",
        "#                 print(f\"Directory '{dirname}' already has the correct name\")\n",
        "\n",
        "# # Lista de diretórios onde estão as imagens\n",
        "# directories = [\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/marked_rgbs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_pngs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_tifs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/rgbs',\n",
        "# ]\n",
        "\n",
        "# # Diretório de imagens com subdiretórios\n",
        "# images_directory = '/content/drive/MyDrive/modulo10/data/dataset_inteli_felipe/images'\n",
        "\n",
        "# # Renomear os arquivos nos diretórios especificados\n",
        "# for directory in directories:\n",
        "#     rename_files(directory)\n",
        "\n",
        "# # Renomear os arquivos no diretório de imagens\n",
        "# rename_files(images_directory)\n",
        "\n",
        "# # Renomear os diretórios de imagens\n",
        "# rename_directories(images_directory)\n",
        "\n",
        "# print(\"Renaming completed in all directories\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOEeanOelgKn"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from PIL import Image, UnidentifiedImageError, ImageEnhance\n",
        "\n",
        "# def rotate_image(image, angle):\n",
        "#     \"\"\"Rotate the image by a specified angle.\"\"\"\n",
        "#     return image.rotate(angle)\n",
        "\n",
        "# def adjust_brightness_contrast(image, brightness_factor, contrast_factor):\n",
        "#     \"\"\"Adjust brightness and contrast of an image.\"\"\"\n",
        "#     enhancer = ImageEnhance.Brightness(image)\n",
        "#     image = enhancer.enhance(brightness_factor)\n",
        "#     enhancer = ImageEnhance.Contrast(image)\n",
        "#     image = enhancer.enhance(contrast_factor)\n",
        "#     return image\n",
        "\n",
        "# # Função para visualizar a imagem rotacionada em 90, 180 e 270 graus\n",
        "# def visualize_specific_rotations(image_path, image_dimension):\n",
        "#     try:\n",
        "#         image = load_img(image_path, target_size=(image_dimension, image_dimension))\n",
        "#         x = img_to_array(image).astype('uint8')\n",
        "#         original_image = Image.fromarray(x)\n",
        "\n",
        "#         rotations = [90, 180, 270]\n",
        "#         fig, axes = plt.subplots(1, len(rotations), figsize=(20, 20))\n",
        "\n",
        "#         for i, angle in enumerate(rotations):\n",
        "#             rotated_image = rotate_image(original_image, angle)\n",
        "#             axes[i].imshow(rotated_image)\n",
        "#             axes[i].set_title(f\"Rotation: {angle} degrees\")\n",
        "#             axes[i].axis('off')\n",
        "\n",
        "#         plt.show()\n",
        "\n",
        "#     except UnidentifiedImageError:\n",
        "#         print(f\"Error: Cannot identify image file {image_path}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Unexpected error occurred with file {image_path}: {e}\")\n",
        "\n",
        "# # Função para visualizar a imagem com brilho e contraste ajustados\n",
        "# def visualize_brightness_contrast(image_path, image_dimension, brightness_factors, contrast_factors):\n",
        "#     try:\n",
        "#         image = load_img(image_path, target_size=(image_dimension, image_dimension))\n",
        "#         x = img_to_array(image).astype('uint8')\n",
        "#         original_image = Image.fromarray(x)\n",
        "\n",
        "#         fig, axes = plt.subplots(len(brightness_factors), len(contrast_factors), figsize=(20, 20))\n",
        "\n",
        "#         for i, brightness in enumerate(brightness_factors):\n",
        "#             for j, contrast in enumerate(contrast_factors):\n",
        "#                 adjusted_image = adjust_brightness_contrast(original_image, brightness, contrast)\n",
        "#                 axes[i, j].imshow(adjusted_image)\n",
        "#                 axes[i, j].set_title(f\"Brightness: {brightness}, Contrast: {contrast}\")\n",
        "#                 axes[i, j].axis('off')\n",
        "\n",
        "#         plt.show()\n",
        "\n",
        "#     except UnidentifiedImageError:\n",
        "#         print(f\"Error: Cannot identify image file {image_path}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Unexpected error occurred with file {image_path}: {e}\")\n",
        "\n",
        "# # Caminho para a imagem de teste\n",
        "# test_image_path = '/content/drive/MyDrive/modulo10/data/575_2019-8-14_S2L1C_21JXJ_TCI.png'\n",
        "# image_dimension = 1200\n",
        "\n",
        "# # Visualizar as rotações específicas na imagem de teste\n",
        "# visualize_specific_rotations(test_image_path, image_dimension)\n",
        "\n",
        "# # Definir fatores de brilho e contraste\n",
        "# brightness_factors = [1.0, 1.5]  # Exemplos de fatores de brilho\n",
        "# contrast_factors = [1.0, 1.5]    # Exemplos de fatores de contraste\n",
        "\n",
        "# # Visualizar os ajustes de brilho e contraste na imagem de teste\n",
        "# visualize_brightness_contrast(test_image_path, image_dimension, brightness_factors, contrast_factors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3AtqoHDocqK"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from PIL import Image, UnidentifiedImageError, ImageEnhance\n",
        "# import numpy as np\n",
        "\n",
        "# def rotate_image(image, angle):\n",
        "#     \"\"\"Rotate the image by a specified angle.\"\"\"\n",
        "#     return image.rotate(angle)\n",
        "\n",
        "# def adjust_brightness_contrast(image, brightness_factor, contrast_factor):\n",
        "#     \"\"\"Adjust brightness and contrast of an image.\"\"\"\n",
        "#     enhancer = ImageEnhance.Brightness(image)\n",
        "#     image = enhancer.enhance(brightness_factor)\n",
        "#     enhancer = ImageEnhance.Contrast(image)\n",
        "#     image = enhancer.enhance(contrast_factor)\n",
        "#     return image\n",
        "\n",
        "# def data_augmentation(directory, image_dimension):\n",
        "#     images_generated = 0\n",
        "#     rotations = [90, 180, 270]\n",
        "#     brightness_factors = [1, 1.5]  # Factors for decreasing and increasing brightness\n",
        "#     contrast_factors = [1, 1.5]    # Factors for decreasing and increasing contrast\n",
        "\n",
        "#     for filename in os.listdir(directory):\n",
        "#         if filename.endswith(\".tif\") or filename.endswith(\".png\"):\n",
        "#             image_path = os.path.join(directory, filename)\n",
        "#             print(f\"Processing file: {filename}\")\n",
        "\n",
        "#             try:\n",
        "#                 image = load_img(image_path, target_size=(image_dimension, image_dimension))\n",
        "#                 x = img_to_array(image)\n",
        "\n",
        "#                 name, ext = os.path.splitext(filename)\n",
        "\n",
        "#                 # Apply rotations\n",
        "#                 for angle in rotations:\n",
        "#                     rotated_image = rotate_image(Image.fromarray(x.astype('uint8')), angle)\n",
        "#                     new_name = f\"{name}_rot{angle}{ext}\"\n",
        "#                     save_path = os.path.join(directory, new_name)\n",
        "#                     rotated_image.save(save_path)\n",
        "#                     images_generated += 1\n",
        "\n",
        "#                     # Apply brightness and contrast adjustments\n",
        "#                     for brightness in brightness_factors:\n",
        "#                         for contrast in contrast_factors:\n",
        "#                             adjusted_image = adjust_brightness_contrast(rotated_image, brightness, contrast)\n",
        "#                             new_name = f\"{name}_rot{angle}_bright{brightness}_cont{contrast}{ext}\"\n",
        "#                             save_path = os.path.join(directory, new_name)\n",
        "#                             adjusted_image.save(save_path)\n",
        "#                             images_generated += 1\n",
        "\n",
        "#             except UnidentifiedImageError:\n",
        "#                 print(f\"Error: Cannot identify image file {image_path}\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Unexpected error occurred with file {filename}: {e}\")\n",
        "\n",
        "#     return images_generated\n",
        "\n",
        "# # Lista de diretórios onde estão as imagens\n",
        "# directories = [\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_pngs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_tifs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/rgbs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/masks'  # Incluindo o diretório de máscaras\n",
        "# ]\n",
        "\n",
        "# image_dimension = 1200\n",
        "\n",
        "# # Aplicar data augmentation em todos os diretórios especificados e contar imagens geradas\n",
        "# total_images_generated = 0\n",
        "# for directory in directories:\n",
        "#     images_generated = data_augmentation(directory, image_dimension)\n",
        "#     total_images_generated += images_generated\n",
        "#     print(f\"Number of images generated in {directory}: {images_generated}\")\n",
        "\n",
        "# print(f\"Total number of images generated in all directories: {total_images_generated}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePCiv_EOjIeW"
      },
      "outputs": [],
      "source": [
        "# image_path = '/content/drive/MyDrive/modulo10/data/dataset_inteli_felipe/tci_tifs/1062_2020-8-8_S2L1C_21JYK_TCI.tif'\n",
        "\n",
        "\n",
        "# with rasterio.open(image_path) as src:\n",
        "#     image_data = src.read(1)\n",
        "\n",
        "# plt.imshow(image_data)\n",
        "# plt.axis('off')\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BCl4qttoV6E"
      },
      "outputs": [],
      "source": [
        "# image_path = '/content/drive/MyDrive/modulo10/data/dataset_inteli_felipe/images/1062_2020-8-8_S2L1C_21JYK/b11.tif'\n",
        "\n",
        "# with rasterio.open(image_path) as src:\n",
        "#     image_data = src.read(1)\n",
        "\n",
        "# plt.imshow(image_data)\n",
        "# plt.axis('off')\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNLDuxzkykEt"
      },
      "source": [
        "## Crop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0MJ664R_emH"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from PIL import Image, UnidentifiedImageError, ImageEnhance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oor8TGxAymec"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "# import os\n",
        "# import uuid\n",
        "\n",
        "# # Diretórios contendo as imagens originais\n",
        "# input_image_dirs = [\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/rgbs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_tifs',\n",
        "#     '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/tci_pngs',\n",
        "# ]\n",
        "# # Diretório contendo as máscaras\n",
        "# input_mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/masks'\n",
        "\n",
        "# # Diretórios onde as imagens e máscaras cortadas serão salvas\n",
        "# output_image_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/newTargetImages2'\n",
        "# output_mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/newTargetMasks2'\n",
        "\n",
        "# os.makedirs(output_image_dir, exist_ok=True)\n",
        "# os.makedirs(output_mask_dir, exist_ok=True)\n",
        "\n",
        "# # Dimensões dos cortes\n",
        "# crop_width, crop_height = 120, 120\n",
        "\n",
        "# # Cortes por imagem\n",
        "# max_cuts = 5\n",
        "\n",
        "# # Função para cortar a imagem e sua máscara em pedaços menores\n",
        "# def crop_image_and_mask(image_path, mask_path, output_image_dir, output_mask_dir, crop_width, crop_height, max_cuts):\n",
        "#     try:\n",
        "#         img = Image.open(image_path)\n",
        "#         mask = Image.open(mask_path)\n",
        "#         img_width, img_height = img.size\n",
        "\n",
        "#         # Gera um nome aleatório para a imagem\n",
        "#         base_name = str(uuid.uuid4())[:8]\n",
        "#         ext = os.path.splitext(image_path)[1]\n",
        "\n",
        "#         cut_count = 0\n",
        "#         for i in range(0, img_width, crop_width):\n",
        "#             for j in range(0, img_height, crop_height):\n",
        "#                 if cut_count >= max_cuts:\n",
        "#                     return\n",
        "#                 box = (i, j, i + crop_width, j + crop_height)\n",
        "#                 cropped_img = img.crop(box)\n",
        "#                 cropped_mask = mask.crop(box)\n",
        "#                 cropped_img_path = os.path.join(output_image_dir, f\"{base_name}_{cut_count+1}{ext}\")\n",
        "#                 cropped_mask_path = os.path.join(output_mask_dir, f\"{base_name}_{cut_count+1}{ext}\")\n",
        "#                 cropped_img.save(cropped_img_path)\n",
        "#                 cropped_mask.save(cropped_mask_path)\n",
        "#                 cut_count += 1\n",
        "#     except Exception as e:\n",
        "#         print(f\"Erro ao processar {image_path}: {e}\")\n",
        "\n",
        "# # Percorrer todos os diretórios de entrada\n",
        "# for input_image_dir in input_image_dirs:\n",
        "#     for filename in os.listdir(input_image_dir):\n",
        "#         if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "#             image_path = os.path.join(input_image_dir, filename)\n",
        "#             mask_path = os.path.join(input_mask_dir, filename.replace('.tif', '.png'))  # Assumindo que as máscaras têm o mesmo nome que as imagens\n",
        "#             if os.path.exists(mask_path):  # Garantindo que a máscara correspondente existe\n",
        "#                 print(f\"Processando {image_path} e {mask_path}\")\n",
        "#                 crop_image_and_mask(image_path, mask_path, output_image_dir, output_mask_dir, crop_width, crop_height, max_cuts)\n",
        "#             else:\n",
        "#                 print(f\"Máscara não encontrada para {image_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHJ9wWNcM-J9"
      },
      "source": [
        "## Modelo de Segmentação de Imagens com U-Net e VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMFrT6LsM7ZV"
      },
      "source": [
        "## Descrição Geral\n",
        "Este notebook implementa um modelo de segmentação de imagens utilizando a arquitetura U-Net com backbone VGG16. A segmentação é realizada em imagens e máscaras carregadas de um diretório específico. A abordagem é útil para tarefas como segmentação de áreas agrícolas em imagens de satélite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-9trdODO42q"
      },
      "source": [
        "## Habilitando GPU no Google Colab\n",
        "Para habilitar a GPU no Google Colab, siga os passos abaixo:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVWO4-fvLPTK"
      },
      "source": [
        "## Importando libs necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URTFHm6FYfbK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import gdown\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Conv2DTranspose\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIX5IUDsQBtK"
      },
      "source": [
        "## Verificando se a GPU está sendo usada\n",
        "Com o código abaixo é possível verificar se a GPU está sendo usada pelo TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt3hXev2Pomg",
        "outputId": "00504e23-c5fa-4b22-c810-0d512c2f0b57"
      },
      "outputs": [],
      "source": [
        "# Verificação da GPU\n",
        "def check_gpu():\n",
        "    \"\"\"Verifica se a GPU está disponível e ativa.\"\"\"\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(f\"GPUs disponíveis: {gpus}\")\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    else:\n",
        "        print(\"Nenhuma GPU disponível.\")\n",
        "\n",
        "    if tf.test.gpu_device_name():\n",
        "        print('GPU ativa:', tf.test.gpu_device_name())\n",
        "    else:\n",
        "        print(\"Nenhuma GPU ativa.\")\n",
        "\n",
        "check_gpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOginfJ4LS35"
      },
      "source": [
        "## Preparação de Dados\n",
        "\n",
        "A preparação de dados envolve baixar e carregar tanto as imagens quanto as máscaras correspondentes, normalizando e transformando esses dados para o formato adequado para treinamento. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas, onde valores acima de 0.5 são considerados 1 (objeto de interesse) e abaixo são 0 (fundo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUTi-Av7bfiz",
        "outputId": "b67d03d5-408d-46c3-d240-317a55f4b207"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf8KRuw6BBGC",
        "outputId": "3fab8e22-bbf4-4698-baa4-b67dacca18f0"
      },
      "outputs": [],
      "source": [
        "def load_images_and_masks_in_batches(data_dir, mask_dir, batch_size=51):\n",
        "    \"\"\"\n",
        "    Carrega imagens e máscaras em batches.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): O diretório contendo as imagens.\n",
        "        mask_dir (str): O diretório contendo as máscaras correspondentes às imagens.\n",
        "        batch_size (int, optional): O tamanho do batch. O padrão é 51.\n",
        "\n",
        "    Yields:\n",
        "        tuple: Um par de arrays numpy contendo os batches de imagens e máscaras.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Se ocorrer um erro ao carregar uma imagem.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Um par de arrays numpy contendo as imagens e máscaras restantes.\n",
        "    \"\"\"\n",
        "    file_names = sorted(os.listdir(data_dir))\n",
        "    images, masks = [], []\n",
        "    count = 0\n",
        "\n",
        "    for file in file_names:\n",
        "        if file.endswith('.png') or file.endswith('.tif'):\n",
        "            img_path = os.path.join(data_dir, file)\n",
        "            mask_path = os.path.join(mask_dir, file)\n",
        "            try:\n",
        "                img = load_img(img_path)\n",
        "                img = img_to_array(img) / 255.0\n",
        "                mask = load_img(mask_path, color_mode='grayscale')\n",
        "                mask = img_to_array(mask) / 255.0\n",
        "                mask = (mask > 0.5).astype(np.float32)\n",
        "\n",
        "                if img.shape == (120, 120, 3) and mask.shape == (120, 120, 1):\n",
        "                    images.append(img)\n",
        "                    masks.append(mask[:, :, 0])\n",
        "                    count += 1\n",
        "                else:\n",
        "                    print(f\"Descartado por dimensões incorretas: {file}\")\n",
        "\n",
        "                if count == batch_size:\n",
        "                    yield np.array(images), np.array(masks)\n",
        "                    images, masks = [], []\n",
        "                    count = 0\n",
        "                    print(f\"Carregou {batch_size} imagens\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao carregar {file}: {e}\")\n",
        "\n",
        "    if images and masks:\n",
        "        yield np.array(images), np.array(masks)\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/newTargetImages2'\n",
        "mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_teste/newTargetMasks2'\n",
        "\n",
        "batch_size = 51\n",
        "all_images, all_masks = [], []\n",
        "\n",
        "for batch_images, batch_masks in load_images_and_masks_in_batches(data_dir, mask_dir, batch_size):\n",
        "    all_images.extend(batch_images)\n",
        "    all_masks.extend(batch_masks)\n",
        "\n",
        "all_images = np.array(all_images)\n",
        "all_masks = np.array(all_masks)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_images, all_masks, test_size=0.4, random_state=42)\n",
        "print(f\"Tamanho do conjunto de treino: {len(X_train)} imagens\")\n",
        "print(f\"Tamanho do conjunto de teste: {len(X_test)} imagens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbmYSI6XuNrN"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# def load_images_and_masks_in_batches(data_dir, mask_dir, batch_size=51):\n",
        "#     file_names = sorted(os.listdir(data_dir))\n",
        "#     images = []\n",
        "#     masks = []\n",
        "#     count = 0\n",
        "\n",
        "#     for file in file_names:\n",
        "#         if file.endswith('.png'):\n",
        "#             img_path = os.path.join(data_dir, file)\n",
        "#             mask_path = os.path.join(mask_dir, file)\n",
        "#             try:\n",
        "#                 img = load_img(img_path)  # Carrega a imagem no formato padrão\n",
        "#                 img = img_to_array(img) / 255.0  # Normaliza a imagem para o intervalo [0, 1]\n",
        "#                 mask = load_img(mask_path, color_mode='grayscale')  # Carrega a máscara como grayscale\n",
        "#                 mask = img_to_array(mask) / 255.0  # Normaliza a máscara\n",
        "#                 mask = (mask > 0.5).astype(np.float32)  # Binariza a máscara\n",
        "\n",
        "#                 if img.shape == (120, 120, 3) and mask.shape == (120, 120, 1):\n",
        "#                     images.append(img)\n",
        "#                     masks.append(mask[:, :, 0])  # Garante que a máscara seja um array 2D\n",
        "#                     count += 1\n",
        "#                 else:\n",
        "#                     print(f\"Descartado por dimensões incorretas: {file}\")\n",
        "\n",
        "#                 if count == batch_size:\n",
        "#                     yield np.array(images), np.array(masks)\n",
        "#                     images, masks = [], []\n",
        "#                     count = 0\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Erro ao carregar {file}: {e}\")\n",
        "\n",
        "#     if images and masks:\n",
        "#         yield np.array(images), np.array(masks)\n",
        "\n",
        "# # Caminhos para os diretórios de imagens e máscaras\n",
        "# data_dir = './data/newTargetImages2'\n",
        "# mask_dir = './data/newTargetMasks2'\n",
        "\n",
        "# # Processar imagens e máscaras em batches\n",
        "# batch_size = 51\n",
        "# all_images = []\n",
        "# all_masks = []\n",
        "\n",
        "# for batch_images, batch_masks in load_images_and_masks_in_batches(data_dir, mask_dir, batch_size):\n",
        "#     all_images.extend(batch_images)\n",
        "#     all_masks.extend(batch_masks)\n",
        "\n",
        "# # Converter listas para arrays numpy\n",
        "# all_images = np.array(all_images)\n",
        "# all_masks = np.array(all_masks)\n",
        "\n",
        "# # Dividir os dados em treino e teste\n",
        "# X_train, X_test, y_train, y_test = train_test_split(all_images, all_masks, test_size=0.1, random_state=42)\n",
        "\n",
        "# print(f\"Tamanho do conjunto de treino: {len(X_train)} imagens\")\n",
        "# print(f\"Tamanho do conjunto de teste: {len(X_test)} imagens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq21Z5gPNLXE"
      },
      "source": [
        "## Funções de perda e métricas personalizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFAwOFAgNM2C"
      },
      "source": [
        "Definimos uma função de perda personalizada que combina a perda Dice e a perda binária de entropia cruzada para otimizar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88A_wQBr1uWZ"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Dice loss between the true labels and predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: The true labels.\n",
        "    - y_pred: The predicted labels.\n",
        "\n",
        "    Returns:\n",
        "    - The Dice loss value.\n",
        "\n",
        "    \"\"\"\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the combined loss by adding binary crossentropy loss and dice loss.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: The true labels.\n",
        "    - y_pred: The predicted labels.\n",
        "\n",
        "    Returns:\n",
        "    The combined loss value.\n",
        "    \"\"\"\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def iou_metric(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the Intersection over Union (IoU) metric for binary segmentation.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: Ground truth binary segmentation mask.\n",
        "    - y_pred: Predicted binary segmentation mask.\n",
        "\n",
        "    Returns:\n",
        "    - IoU metric value.\n",
        "\n",
        "    \"\"\"\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    return intersection / (union + tf.keras.backend.epsilon())\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the precision metric for binary classification.\n",
        "\n",
        "    Precision is the ratio of true positives to the sum of true positives and false positives.\n",
        "    It measures the ability of the model to correctly predict positive samples.\n",
        "\n",
        "    Args:\n",
        "        y_true (tensor): True labels.\n",
        "        y_pred (tensor): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        tensor: Precision value.\n",
        "\n",
        "    \"\"\"\n",
        "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n",
        "    return true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the recall metric for a binary classification problem.\n",
        "\n",
        "    Recall measures the ability of a model to find all the relevant cases (true positives) in a dataset.\n",
        "\n",
        "    Args:\n",
        "        y_true (tensor): True labels of the data.\n",
        "        y_pred (tensor): Predicted labels of the data.\n",
        "\n",
        "    Returns:\n",
        "        tensor: The recall score.\n",
        "\n",
        "    \"\"\"\n",
        "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the F1 score, which is a measure of a model's accuracy.\n",
        "\n",
        "    The F1 score is the harmonic mean of precision and recall. It is a useful metric\n",
        "    for imbalanced datasets, where the number of samples in different classes is\n",
        "    significantly different.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (array-like): The true labels.\n",
        "        y_pred (array-like): The predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        float: The F1 score.\n",
        "    \"\"\"\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "\n",
        "def coefficient_of_variation_of_coverage(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the coefficient of variation of coverage.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: The true values.\n",
        "    - y_pred: The predicted values.\n",
        "\n",
        "    Returns:\n",
        "    The coefficient of variation of coverage.\n",
        "    \"\"\"\n",
        "    mean_coverage = tf.reduce_mean(y_pred)\n",
        "    std_deviation = tf.math.reduce_std(y_pred)\n",
        "    cvr = std_deviation / (mean_coverage + tf.keras.backend.epsilon())\n",
        "\n",
        "    return cvr\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the accuracy of the predicted values.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: The true values.\n",
        "    - y_pred: The predicted values.\n",
        "\n",
        "    Returns:\n",
        "    - The accuracy of the predicted values.\n",
        "    \"\"\"\n",
        "    y_pred = tf.round(y_pred)\n",
        "    y_true = tf.round(y_true)\n",
        "    return tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32)) / tf.reduce_sum(tf.cast(tf.size(y_true), tf.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCVCgzJbNQMo"
      },
      "source": [
        "## Arquitetura do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG-nM2G4ShEn"
      },
      "source": [
        "### Descrição Do Modelo Utilizado - Estrutura Geral\n",
        "\n",
        "O modelo U-Net é uma rede neural convolucional projetada para tarefas de segmentação de imagem. A arquitetura é composta por duas partes principais: o codificador (encoder) e o decodificador (decoder). A estrutura do U-Net permite capturar tanto o contexto global quanto os detalhes locais da imagem. O U-Net, proposto por Ronneberger et al. (2015), destaca-se pelo uso de conexões de skip entre o encoder e o decoder, que preservam informações de alta resolução essenciais para segmentação precisa. Este modelo é amplamente utilizado em diversas aplicações, incluindo a segmentação de tumores e análise de imagens médicas.\n",
        "\n",
        "Referências:\n",
        "\n",
        "Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234-241).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8uxCnb-SjOl"
      },
      "source": [
        "### Codificador (Encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAdjAsIjSrJf"
      },
      "source": [
        "\n",
        "O codificador é baseado na arquitetura MobileNetV2 pré-treinada no conjunto de dados ImageNet. Ele consiste em uma série de camadas convolucionais projetadas para extrair características da imagem de entrada em diferentes níveis de abstração. As camadas utilizadas são:\n",
        "\n",
        "- **block_1_expand_relu:**\n",
        "  - Produz uma saída de 64x64.\n",
        "\n",
        "- **block_3_expand_relu:**\n",
        "  - Produz uma saída de 32x32.\n",
        "\n",
        "- **block_6_expand_relu:**\n",
        "  - Produz uma saída de 16x16.\n",
        "\n",
        "- **block_13_expand_relu:**\n",
        "  - Produz uma saída de 8x8.\n",
        "\n",
        "- **Saída final do MobileNetV2:**\n",
        "  - Produz uma saída de 4x4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMYx7ILYSs-X"
      },
      "source": [
        "### Decodificador (Decoder)\n",
        "\n",
        "O decodificador reconstrói a imagem de saída a partir das características extraídas pelo codificador. Utiliza operações de upsampling e camadas convolucionais para aumentar a resolução da imagem. As camadas do decodificador incluem:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTxvbbcJStoe"
      },
      "source": [
        "- **UpSampling 1:**\n",
        "  - Conv2DTranspose com 128 filtros e kernel 2x2\n",
        "  - Resizing para ajustar a saída ao tamanho correspondente do encoder\n",
        "  - concatenate com a saída block_13_expand_relu\n",
        "  - Dropout com taxa definida\n",
        "  - Conv2D com 128 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 2:**\n",
        "  - Conv2DTranspose com 64 filtros e kernel 2x2\n",
        "  - Resizing para ajustar a saída ao tamanho correspondente do encoder\n",
        "  - concatenate com a saída block_6_expand_relu\n",
        "  - Dropout com taxa definida\n",
        "  - Conv2D com 64 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 3:**\n",
        "  - Conv2DTranspose com 32 filtros e kernel 2x2\n",
        "  - Resizing para ajustar a saída ao tamanho correspondente do encoder\n",
        "  - concatenate com a saída block_3_expand_relu\n",
        "  - Dropout com taxa definida\n",
        "  - Conv2D com 32 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 4:**\n",
        "  - Conv2DTranspose com 16 filtros e kernel 2x2\n",
        "  - Resizing para ajustar a saída ao tamanho correspondente do encoder\n",
        "  - concatenate com a saída block_1_expand_relu\n",
        "  - Dropout com taxa definida\n",
        "  - Conv2D com 16 filtros e kernel 3x3, ativação ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HbkNxL9SwhP"
      },
      "source": [
        "\n",
        "### Saída (Output)\n",
        "\n",
        "A última camada do decodificador é uma Conv2D com um único filtro e função de ativação sigmoid, produzindo a máscara segmentada binária. A camada Resizing garante que a saída tenha o mesmo tamanho da entrada, ajustando para (120, 120).\n",
        "\n",
        "Esta arquitetura combina a eficiência do MobileNetV2 para extração de características com a capacidade da U-Net de realizar segmentação precisa, sendo adequada para diversas aplicações em segmentação de imagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6P2ihn0vWzx",
        "outputId": "26d379e8-bfab-415d-cf0f-35b5b21b7682"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, concatenate, Dropout, Resizing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "def unet_with_mobilenet_backbone(input_size=(120, 120, 3), dropout_rate=0.1):\n",
        "    \"\"\"\n",
        "    Creates a U-Net model with MobileNetV2 as the backbone.\n",
        "\n",
        "    Args:\n",
        "        input_size (tuple): The input size of the model in the format (height, width, channels).\n",
        "        dropout_rate (float): The dropout rate to be applied after each concatenation layer.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The U-Net model with MobileNetV2 backbone.\n",
        "\n",
        "    \"\"\"\n",
        "    # Carrega o MobileNetV2 como backbone, sem a parte superior e com pesos pré-treinados do ImageNet\n",
        "    mobilenet = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "\n",
        "    # Congelar camadas do backbone para preservar os pesos pré-treinados\n",
        "    for layer in mobilenet.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Extração de features em diferentes níveis\n",
        "    s1 = mobilenet.get_layer('block_1_expand_relu').output   # 64x64\n",
        "    s2 = mobilenet.get_layer('block_3_expand_relu').output   # 32x32\n",
        "    s3 = mobilenet.get_layer('block_6_expand_relu').output   # 16x16\n",
        "    s4 = mobilenet.get_layer('block_13_expand_relu').output  # 8x8\n",
        "    b1 = mobilenet.output  # 4x4\n",
        "\n",
        "    # Decoder com redução do número de filtros e simplificação das operações\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(b1)\n",
        "    u6 = Resizing(s4.shape[1], s4.shape[2])(u6)\n",
        "    u6 = concatenate([u6, s4])\n",
        "    u6 = Dropout(dropout_rate)(u6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(u6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = Resizing(s3.shape[1], s3.shape[2])(u7)\n",
        "    u7 = concatenate([u7, s3])\n",
        "    u7 = Dropout(dropout_rate)(u7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(u7)\n",
        "\n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = Resizing(s2.shape[1], s2.shape[2])(u8)\n",
        "    u8 = concatenate([u8, s2])\n",
        "    u8 = Dropout(dropout_rate)(u8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', padding='same')(u8)\n",
        "\n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = Resizing(s1.shape[1], s1.shape[2])(u9)\n",
        "    u9 = concatenate([u9, s1])\n",
        "    u9 = Dropout(dropout_rate)(u9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', padding='same')(u9)\n",
        "\n",
        "    # Camada de saída com resizing para garantir que a saída tem o tamanho de entrada\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "    outputs = Resizing(input_size[0], input_size[1])(outputs)\n",
        "\n",
        "    model = Model(inputs=mobilenet.input, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = unet_with_mobilenet_backbone()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xio4Y4nOlLlX"
      },
      "outputs": [],
      "source": [
        "def create_model(dropout_rate):\n",
        "    \"\"\"\n",
        "    Creates a model with a specified dropout rate.\n",
        "\n",
        "    Parameters:\n",
        "    - dropout_rate (float): The dropout rate to be used in the model.\n",
        "\n",
        "    Returns:\n",
        "    - model: The compiled model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = unet_with_mobilenet_backbone(dropout_rate=dropout_rate)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            precision,\n",
        "            recall,\n",
        "            f1_score,\n",
        "            iou_metric,\n",
        "            coefficient_of_variation_of_coverage\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agzNTC4caLZQ"
      },
      "source": [
        "## Avaliando o melhor valor de Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcmnygo1vI2B",
        "outputId": "a3ccb75a-a28a-4a6a-b5d4-710fb85aea6d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test, epochs=30, model_path='best_model.h5'):\n",
        "    \"\"\"\n",
        "    Trains a given model using the provided training and testing data.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The model to be trained.\n",
        "    - X_train: The input features of the training data.\n",
        "    - y_train: The target labels of the training data.\n",
        "    - X_test: The input features of the testing data.\n",
        "    - y_test: The target labels of the testing data.\n",
        "    - epochs: The number of epochs to train the model (default: 30).\n",
        "    - model_path: The path to save the best model (default: 'best_model.h5').\n",
        "\n",
        "    Returns:\n",
        "    - history: The training history of the model.\n",
        "    \"\"\"\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        verbose=1,\n",
        "        mode='min',\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        model_path,\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[early_stopping, model_checkpoint]\n",
        "    )\n",
        "    return history\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_dropout = 0\n",
        "model_path = 'best_model.h5'\n",
        "\n",
        "dropouts = np.concatenate((np.arange(0.01, 0.1, 0.01), np.arange(0.1, 0.6, 0.1)))\n",
        "val_losses = []\n",
        "dropout_values = []\n",
        "\n",
        "for dropout in dropouts:\n",
        "    print(f\"Testing model with {dropout*100:.1f}% dropout rate...\")\n",
        "    model = create_model(dropout_rate=dropout)\n",
        "    history = train_model(model, X_train, y_train, X_test, y_test, epochs=100, model_path=model_path)\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    val_losses.append(val_loss)\n",
        "    dropout_values.append(dropout)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_dropout = dropout\n",
        "        best_model_path = model_path\n",
        "\n",
        "    print(f\"Minimum validation loss for {dropout*100:.1f}% dropout: {val_loss:.4f}\")\n",
        "\n",
        "custom_objects = {\n",
        "    'binary_crossentropy': tf.keras.losses.binary_crossentropy,\n",
        "    'dice_loss': dice_loss,\n",
        "    'combined_loss': combined_loss,\n",
        "    'iou_metric': iou_metric,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1_score': f1_score,\n",
        "    'coefficient_of_variation_of_coverage': coefficient_of_variation_of_coverage,\n",
        "    'accuracy': accuracy\n",
        "}\n",
        "\n",
        "\n",
        "best_model = tf.keras.models.load_model(best_model_path, custom_objects=custom_objects)\n",
        "print(f\"Best model uses dropout rate: {best_dropout*100:.1f}%, with validation loss: {best_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "Tdq3kH1UnT_1",
        "outputId": "810dc869-7309-46e0-ad11-1afe9680d4ae"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_validation_loss(dropout_values, val_losses):\n",
        "    \"\"\"\n",
        "    Plots the validation loss against the dropout rate.\n",
        "\n",
        "    Parameters:\n",
        "    - dropout_values (list): List of dropout rates.\n",
        "    - val_losses (list): List of validation losses.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(dropout_values, val_losses, marker='o', linestyle='-', color='b', label='Validation Loss')\n",
        "    plt.title('Validation Loss vs. Dropout Rate', fontsize=14)\n",
        "    plt.xlabel('Dropout Rate (%)', fontsize=12)\n",
        "    plt.ylabel('Validation Loss', fontsize=12)\n",
        "    plt.xticks(dropout_values, labels=[f\"{rate*100:.1f}%\" for rate in dropout_values])\n",
        "    plt.yticks(np.arange(min(val_losses), max(val_losses), 0.005))\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MIGLR1oBgMO"
      },
      "source": [
        "## Avaliação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHXfUd91xUQj",
        "outputId": "e51092e2-78ad-44bf-b878-17d29f791c5c"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Avalia o modelo treinado no conjunto de teste.\n",
        "\n",
        "    Parameters:\n",
        "    model (object): O modelo treinado a ser avaliado.\n",
        "    X_test (array-like): Os dados de teste.\n",
        "    y_test (array-like): Os rótulos de teste.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    model.evaluate(X_test, y_test, batch_size=1)\n",
        "    model.save('segmentation_model.h5')\n",
        "\n",
        "evaluate_model(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhoCwWThNqvF"
      },
      "source": [
        "## Plotando métricas\n",
        "Plotamos as métricas de perda e acurácia do modelo durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "t_Ee08KIKwot",
        "outputId": "8fe4a4d1-da5b-4a53-bf83-406d7226d122"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(history):\n",
        "    \"\"\"\n",
        "    Plots the loss and accuracy metrics of the model during training.\n",
        "\n",
        "    Parameters:\n",
        "    history (object): The history object returned by the `fit` method of a Keras model.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Plotting the loss\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(history.history['loss'], label='Train')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the accuracy\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the IOU metric\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(history.history['iou_metric'], label='Train')\n",
        "    plt.plot(history.history['val_iou_metric'], label='Validation')\n",
        "    plt.title('IOU')\n",
        "    plt.ylabel('IOU')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the precision\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(history.history['precision'], label='Train')\n",
        "    plt.plot(history.history['val_precision'], label='Validation')\n",
        "    plt.title('Precision')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the recall\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(history.history['recall'], label='Train')\n",
        "    plt.plot(history.history['val_recall'], label='Validation')\n",
        "    plt.title('Recall')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the F1 score\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(history.history['f1_score'], label='Train')\n",
        "    plt.plot(history.history['val_f1_score'], label='Validation')\n",
        "    plt.title('F1 Score')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the COVR\n",
        "    plt.subplot(2, 4, 7)\n",
        "    plt.plot(history.history['coefficient_of_variation_of_coverage'], label='Treino')\n",
        "    plt.plot(history.history['val_coefficient_of_variation_of_coverage'], label='Validação')\n",
        "    plt.title('COVR')\n",
        "    plt.ylabel('COVR')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK-BDvBlNwXp"
      },
      "source": [
        "## Função de Pós-Processamento para Máscaras de Segmentação\n",
        "\n",
        "Realização do pós-processamento das máscaras binárias geradas. O objetivo é suavizar, preencher quando possível e arredondar as bordas da máscara, melhorando a aparência visual das previsões e aumentando a precisão prática da segmentação.\n",
        "\n",
        "### Parâmetros\n",
        "\n",
        "- `mask (np.array)`: A máscara binária original predita pelo modelo.\n",
        "- `kernel_size (int)`: O tamanho do kernel utilizado para as operações morfológicas. Um kernel maior intensifica os efeitos de erosão e dilatação.\n",
        "\n",
        "### Processo\n",
        "\n",
        "1. **Erosão**: A erosão é aplicada primeiro para eliminar pequenos ruídos e separar objetos que estão levemente conectados. Isso ajuda a clarificar a máscara e reduzir artefatos indesejados.\n",
        "\n",
        "2. **Dilatação**: Após a erosão, a dilatação é aplicada para restaurar o tamanho dos objetos que foram erodidos, ao mesmo tempo que mantém as desconexões introduzidas pela erosão. Isso ajuda a preservar a integridade estrutural dos objetos na máscara, enquanto ainda suaviza as bordas.\n",
        "\n",
        "<!-- ### Exemplo de Código\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def post_process_mask(mask, kernel_size=3):\n",
        "    \"\"\"\n",
        "    Aplica erosão e dilatação para suavizar as bordas da máscara.\n",
        "    \"\"\"\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    erosion = cv2.erode(mask, kernel, iterations=1)\n",
        "    dilation = cv2.dilate(erosion, kernel, iterations=1)\n",
        "    return dilation -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qiZxl2sn0vlk",
        "outputId": "11f1f4e6-de08-434f-a04f-444fe8728f64"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def post_process_mask(mask, kernel_size=3):\n",
        "    \"\"\"\n",
        "    Applies erosion followed by dilation to smooth the edges of the mask.\n",
        "\n",
        "    Parameters:\n",
        "    - mask: The input mask image.\n",
        "    - kernel_size: The size of the kernel used for erosion and dilation. Default is 3.\n",
        "\n",
        "    Returns:\n",
        "    The processed mask image.\n",
        "    \"\"\"\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    erosion = cv2.erode(mask, kernel, iterations=1)\n",
        "    dilation = cv2.dilate(erosion, kernel, iterations=1)\n",
        "    return dilation\n",
        "\n",
        "def plot_results(X, y, model, num_images=40):\n",
        "    \"\"\"\n",
        "    Plots the image, real mask, predicted mask, and post-processed mask for a specified number of images.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Array of input images.\n",
        "    - y (numpy.ndarray): Array of corresponding masks.\n",
        "    - model: The trained model used for prediction.\n",
        "    - num_images (int): Number of images to plot (default is 40).\n",
        "    \"\"\"\n",
        "    for i in range(num_images):\n",
        "        ix = np.random.randint(0, len(X))\n",
        "        fig, ax = plt.subplots(1, 4, figsize=(27, 10))\n",
        "\n",
        "        # Plotting the original image\n",
        "        ax[0].imshow(X[ix], cmap='gray')\n",
        "        ax[0].title.set_text('Original Image')\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        # Plotting the real mask\n",
        "        ax[1].imshow(y[ix].squeeze(), cmap='gray')\n",
        "        ax[1].title.set_text('Real Mask')\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        # Making the prediction mask and plotting\n",
        "        pred = model.predict(X[ix:ix+1])\n",
        "        pred = (pred > 0.5).astype(np.float32)\n",
        "        ax[2].imshow(pred.squeeze(), cmap='gray')\n",
        "        ax[2].title.set_text('Predicted Mask')\n",
        "        ax[2].axis('off')\n",
        "\n",
        "        # Applying post-processing to the predicted mask and plotting\n",
        "        pred_processed = post_process_mask(pred.squeeze())\n",
        "        ax[3].imshow(pred_processed, cmap='gray')\n",
        "        ax[3].title.set_text('Post-processed Mask')\n",
        "        ax[3].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "plot_results(X_test, y_test, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHaL0mREaLZS"
      },
      "source": [
        "## Função para recontrução da imagem 1200 x 1200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFZynTwIvVS6"
      },
      "source": [
        "Não conseguimos implementar 100% essa etapa mas segue abaixo uma das tentativas de funções para resolver esse problema. Acreditamos que seja útil para futuros desenvolvimentos desse projeto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR3_RYoKaLZS"
      },
      "outputs": [],
      "source": [
        "# import Image\n",
        "\n",
        "# # Função para fazer a predição em uma imagem cortada\n",
        "# def predict_image(image_path, model):\n",
        "#     image = Image.open(image_path).resize((1200, 1200))\n",
        "#     image_array = np.array(image) / 255.0\n",
        "#     image_array = np.expand_dims(image_array, axis=0)  # Adicionar batch dimension\n",
        "#     prediction = model.predict(image_array)\n",
        "#     prediction_image = np.squeeze(prediction)  # Remover batch dimension\n",
        "#     prediction_image = (prediction_image * 255).astype(np.uint8)\n",
        "#     return Image.fromarray(prediction_image)\n",
        "\n",
        "# # Caminho para as partes cortadas da imagem\n",
        "# image_parts_dir = '/content/drive/MyDrive/modulo10/data/cropped_images'\n",
        "\n",
        "# # Obter lista de partes da imagem\n",
        "# image_parts = sorted(os.listdir(image_parts_dir))\n",
        "\n",
        "# # Inicializar uma imagem em branco para recompor a imagem completa\n",
        "# image_size = 1200\n",
        "# n_parts = int(np.sqrt(len(image_parts)))  # Supondo um corte quadrado das imagens\n",
        "# full_image = Image.new('RGB', (image_size * n_parts, image_size * n_parts))\n",
        "\n",
        "# # Recompor a imagem completa\n",
        "# for i, part in enumerate(image_parts):\n",
        "#     part_path = os.path.join(image_parts_dir, part)\n",
        "#     prediction = predict_image(part_path, model)\n",
        "#     x = (i % n_parts) * image_size\n",
        "#     y = (i // n_parts) * image_size\n",
        "#     full_image.paste(prediction, (x, y))\n",
        "\n",
        "# # Salvar ou exibir a imagem completa com as predições\n",
        "# full_image.save('/content/drive/MyDrive/modulo10/data/full_image_with_predictions.png')\n",
        "# full_image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L1XU9wMQhJm"
      },
      "source": [
        "## Instruções para Obtenção do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2x93e1RHtG"
      },
      "source": [
        "### Passo 1: Aquisição dos Dados\n",
        "Os dados utilizados neste exemplo são imagens e máscaras que podem ser carregadas a partir de um diretório local ou de uma unidade do Google Drive. Certifique-se de que as imagens e as máscaras estejam no formato PNG e que as máscaras sejam imagens em escala de cinza (grayscale).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzS5_oPKRLlI"
      },
      "source": [
        "### Passo 2: Configuração do Ambiente\n",
        "Para utilizar a GPU no Google Colab:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz4h5XbuRRRm"
      },
      "source": [
        "### Passo 3: Preparação dos Dados\n",
        "Os dados devem ser normalizados e divididos em conjuntos de treino e teste. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas (valores acima de 0.5 são considerados 1 e abaixo são 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C9mEoQzRTM3"
      },
      "source": [
        "### Passo 4: Treinamento do Modelo\n",
        "O modelo U-Net com backbone VGG16 é treinado utilizando os dados preparados. Ajuste os hiperparâmetros como número de épocas, tamanho do lote e taxa de aprendizado conforme necessário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apMd2GleRVS3"
      },
      "source": [
        "### Passo 5: Avaliação do Modelo\n",
        "Após o treinamento, o modelo é avaliado no conjunto de teste para medir seu desempenho. Métricas como a perda e a acurácia são plotadas para análise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIipLNtORXd1"
      },
      "source": [
        "\n",
        "### Passo 6: Refinamento do Modelo\n",
        "Se o desempenho do modelo não for satisfatório, existem as seguintes estratégias de refinamento:\n",
        "1. **Ajuste de Hiperparâmetros:** Modificar a taxa de aprendizado (learning rate ou `lr`), o número de épocas (`epochs`), o tamanho do lote (`batch_size`), etc.\n",
        "2. **Aumento de Dados (Data Augmentation):** Utilize técnicas de aumento de dados para gerar mais exemplos de treino, como rotações, zoom, translações, etc.\n",
        "3. **Arquitetura do Modelo:** Modificar a arquitetura do modelo, adicionando mais camadas ou unidades.\n",
        "4. **Regularização:** Adicionar camadas de Dropout ou ajuste a regularização L2 para evitar overfitting.\n",
        "5. **Treinamento com Mais Dados:** Adicionar mais dados de treino para melhorar a generalização do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWuxBRdjVHX1"
      },
      "source": [
        "## Instruções para deploy em um serviço em nuvem\n",
        "\n",
        "As seguintes instruções fornecem um guia para preparar, fazer upload e fazer o deploy do modelo de segmentação no AWS SageMaker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWkVb-N8VREL"
      },
      "source": [
        "### 1. Preparação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3NcQ-b2VUZB"
      },
      "source": [
        "Instale a AWS CLI e configure suas credenciais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gtlogVwVXf1"
      },
      "outputs": [],
      "source": [
        "pip install awscli\n",
        "aws configure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXLEP-PnVb90"
      },
      "source": [
        "Instale o SDK do SageMaker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYbsd_LIVeQy"
      },
      "outputs": [],
      "source": [
        "pip install sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHHcJiCbVpeJ"
      },
      "source": [
        "### 2. Preparar o notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuy83-awVsgB"
      },
      "source": [
        "Certifique-se que todas as dependências do notebook estão instaladas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ7TMYdcVwDv"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas scikit-learn matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOEPgPZtVy99"
      },
      "source": [
        "### 3. Preparação do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqk3zq_-V1uI"
      },
      "source": [
        "Após o treinamento do modelo, salve-o no formato .h5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6P1BlSfV4x1"
      },
      "outputs": [],
      "source": [
        "model.save('segmentation_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFU67brCV_Tf"
      },
      "source": [
        "Após isso, carregue o arquivo do modelo salvo para um bucket no S3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTSlWjGkV_23"
      },
      "outputs": [],
      "source": [
        "aws s3 cp segmentation_model.h5 s3://nome-do-bucket/modelos/segmentation_model.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52a6LCNYWCVT"
      },
      "source": [
        "### 4. Deploy no SageMaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aXrl4h1WEfs"
      },
      "source": [
        "Crie um arquivo inference.py e coloque o seguinte código que contém a lógica de carregamento do modelo e inferência:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHAsTyUiWHrO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    model = load_model(f'{model_dir}/segmentation_model.h5')\n",
        "    return model\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    if request_content_type == 'application/json':\n",
        "        return np.array(json.loads(request_body)['instances'])\n",
        "    else:\n",
        "        raise ValueError('Esse modelo apenas suporta JSON')\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    predictions = model.predict(input_data)\n",
        "    return predictions\n",
        "\n",
        "def output_fn(prediction, response_content_type):\n",
        "    return json.dumps({'predições': prediction.tolist()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PnqpnU6WLcA"
      },
      "source": [
        "Utilize o SDK do SageMaker para criar um endpoint para o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUd4lip4WOqw"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "from sagemaker.tensorflow import TensorFlowModel\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = 'arn:aws:iam::sua-conta:permissao/sagemaker-permissao'\n",
        "\n",
        "model = TensorFlowModel(model_data='s3://nome-bucket/modelos/segmentation_model.h5',\n",
        "                        role=role,\n",
        "                        entry_point='inference.py',\n",
        "                        framework_version='2.3',\n",
        "                        sagemaker_session=sagemaker_session)\n",
        "\n",
        "predictor = model.deploy(initial_instance_count=1,\n",
        "                         instance_type='ml.m4.xlarge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0VX5XaxWTg7"
      },
      "source": [
        "### 5. Teste e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlq8_fsjWWvE"
      },
      "source": [
        "Envie uma requisição de teste ao endpoint para garantir que tudo esteja funcionando corretamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmCcROXTWaev"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import boto3\n",
        "\n",
        "runtime = boto3.client('runtime.sagemaker')\n",
        "payload = json.dumps({'instances': np.random.rand(1, 224, 224, 3).tolist()})\n",
        "\n",
        "response = runtime.invoke_endpoint(EndpointName=predictor.endpoint_name,\n",
        "                                   ContentType='application/json',\n",
        "                                   Body=payload)\n",
        "\n",
        "result = json.loads(response['Body'].read().decode())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAf8p-c-Whcn"
      },
      "source": [
        "### 6. Limpeza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlN_HqhTWj-A"
      },
      "source": [
        "Para evitar custos desnecessários, remova os recursos quando terminar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDKvFD0nWrKx"
      },
      "outputs": [],
      "source": [
        "predictor.delete_endpoint()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
