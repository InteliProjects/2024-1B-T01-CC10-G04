{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_g2byv26mzB",
        "outputId": "013084b6-2502-46ab-b986-1536a70a3efe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0kvPP3jZlKX"
      },
      "source": [
        "# Pré-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcB76RrcugkZ"
      },
      "source": [
        "## Crop das Imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4j2x3b4ullD"
      },
      "outputs": [],
      "source": [
        "# diretório que contém as imagens originais\n",
        "input_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/masks'\n",
        "\n",
        "# diretório onde as imagens cortadas serão salvas\n",
        "output_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/testeImagensCortadasMasks'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# dimensões dos cortes\n",
        "crop_width, crop_height = 120, 120\n",
        "\n",
        "# método para cortar uma imagem em pedaços menores\n",
        "def crop_image(image_path, output_dir, crop_width, crop_height):\n",
        "    img = Image.open(image_path)\n",
        "    img_width, img_height = img.size\n",
        "\n",
        "    for i in range(0, img_width, crop_width):\n",
        "        for j in range(0, img_height, crop_height):\n",
        "            box = (i, j, i + crop_width, j + crop_height)\n",
        "            cropped_img = img.crop(box)\n",
        "            cropped_img_path = os.path.join(output_dir, f\"{os.path.basename(image_path).split('.')[0]}_{i}_{j}.png\")\n",
        "            cropped_img.save(cropped_img_path)\n",
        "\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "        image_path = os.path.join(input_dir, filename)\n",
        "        crop_image(image_path, output_dir, crop_width, crop_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVdxIME1uljC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "# diretórios contendo as imagens originais\n",
        "input_image_dirs = [\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/marked_rgbs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/rgbs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/tci_tifs',\n",
        "    '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/tci_pngs',\n",
        "]\n",
        "# diretório contendo as máscaras\n",
        "input_mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/masks'\n",
        "\n",
        "# diretórios onde as imagens e máscaras cortadas serão salvas\n",
        "output_image_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/newTargetImages2'\n",
        "output_mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/newTargetMasks2'\n",
        "\n",
        "os.makedirs(output_image_dir, exist_ok=True)\n",
        "os.makedirs(output_mask_dir, exist_ok=True)\n",
        "\n",
        "# dimensões dos cortes\n",
        "crop_width, crop_height = 120, 120\n",
        "\n",
        "# cortes por imagem\n",
        "max_cuts = 5\n",
        "\n",
        "# função para cortar a imagem e sua máscara em pedaços menores\n",
        "def crop_image_and_mask(image_path, mask_path, output_image_dir, output_mask_dir, crop_width, crop_height, max_cuts):\n",
        "    img = Image.open(image_path)\n",
        "    mask = Image.open(mask_path)\n",
        "    img_width, img_height = img.size\n",
        "\n",
        "    # gera um nome aleatório para a imagem\n",
        "    base_name = str(uuid.uuid4())[:8]\n",
        "\n",
        "    ext = os.path.splitext(image_path)[1]\n",
        "\n",
        "    cut_count = 0\n",
        "    for i in range(0, img_width, crop_width):\n",
        "        for j in range(0, img_height, crop_height):\n",
        "            if cut_count >= max_cuts:\n",
        "                return\n",
        "            box = (i, j, i + crop_width, j + crop_height)\n",
        "            cropped_img = img.crop(box)\n",
        "            cropped_mask = mask.crop(box)\n",
        "            cropped_img_path = os.path.join(output_image_dir, f\"{base_name}_{cut_count+1}{ext}\")\n",
        "            cropped_mask_path = os.path.join(output_mask_dir, f\"{base_name}_{cut_count+1}{ext}\")\n",
        "            cropped_img.save(cropped_img_path)\n",
        "            cropped_mask.save(cropped_mask_path)\n",
        "            cut_count += 1\n",
        "\n",
        "# percorrer todos os diretórios de entrada\n",
        "for input_image_dir in input_image_dirs:\n",
        "    for filename in os.listdir(input_image_dir):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "            image_path = os.path.join(input_image_dir, filename)\n",
        "            mask_path = os.path.join(input_mask_dir, filename)  # assumindo que as máscaras têm o mesmo nome que as imagens\n",
        "            if os.path.exists(mask_path):  # garantindo que a máscara correspondente existe\n",
        "                crop_image_and_mask(image_path, mask_path, output_image_dir, output_mask_dir, crop_width, crop_height, max_cuts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHJ9wWNcM-J9"
      },
      "source": [
        "# Modelo de Segmentação de Imagens com U-Net e VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMFrT6LsM7ZV"
      },
      "source": [
        "## Descrição Geral\n",
        "Este notebook implementa um modelo de segmentação de imagens utilizando a arquitetura U-Net com backbone VGG16. A segmentação é realizada em imagens e máscaras carregadas de um diretório específico. A abordagem é útil para tarefas como segmentação de áreas agrícolas em imagens de satélite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-9trdODO42q"
      },
      "source": [
        "## Habilitando GPU no Google Colab\n",
        "Para habilitar a GPU no Google Colab, siga os passos abaixo:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVWO4-fvLPTK"
      },
      "source": [
        "## Importando libs necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URTFHm6FYfbK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIX5IUDsQBtK"
      },
      "source": [
        "## Verificando se a GPU está sendo usada\n",
        "Com o código abaixo é possível verificar se a GPU está sendo usada pelo TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt3hXev2Pomg",
        "outputId": "115ff004-162b-4b76-c7e8-a03e1ea8d844"
      },
      "outputs": [],
      "source": [
        "# Verifica se a GPU está disponível\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"GPUs disponíveis: {gpus}\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"Nenhuma GPU disponível.\")\n",
        "\n",
        "# Verifica se o TensorFlow está utilizando a GPU\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU ativa:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"Nenhuma GPU ativa.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOginfJ4LS35"
      },
      "source": [
        "## Preparação de Dados\n",
        "\n",
        "A preparação de dados envolve carregar as imagens e as máscaras correspondentes, normalizando e transformando esses dados para o formato adequado para treinamento. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas, onde valores acima de 0.5 são considerados 1 (objeto de interesse) e abaixo são 0 (fundo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbmYSI6XuNrN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_images_and_masks_in_batches(data_dir, mask_dir, batch_size=51):\n",
        "    file_names = sorted(os.listdir(data_dir))\n",
        "    images = []\n",
        "    masks = []\n",
        "    count = 0\n",
        "\n",
        "    for file in file_names:\n",
        "        if file.endswith('.png'):\n",
        "            img_path = os.path.join(data_dir, file)\n",
        "            mask_path = os.path.join(mask_dir, file)\n",
        "            try:\n",
        "                img = load_img(img_path)  # Carrega a imagem no formato padrão\n",
        "                img = img_to_array(img) / 255.0  # Normaliza a imagem para o intervalo [0, 1]\n",
        "                mask = load_img(mask_path, color_mode='grayscale')  # Carrega a máscara como grayscale\n",
        "                mask = img_to_array(mask) / 255.0  # Normaliza a máscara\n",
        "                mask = (mask > 0.5).astype(np.float32)  # Binariza a máscara\n",
        "\n",
        "                if img.shape == (120, 120, 3) and mask.shape == (120, 120, 1):\n",
        "                    images.append(img)\n",
        "                    masks.append(mask[:, :, 0])  # Garante que a máscara seja um array 2D\n",
        "                    count += 1\n",
        "                else:\n",
        "                    print(f\"Descartado por dimensões incorretas: {file}\")\n",
        "\n",
        "                if count == batch_size:\n",
        "                    yield np.array(images), np.array(masks)\n",
        "                    images, masks = [], []\n",
        "                    count = 0\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao carregar {file}: {e}\")\n",
        "\n",
        "    if images and masks:\n",
        "        yield np.array(images), np.array(masks)\n",
        "\n",
        "# Caminhos para os diretórios de imagens e máscaras\n",
        "data_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/newTargetImages2'\n",
        "mask_dir = '/content/drive/MyDrive/modulo10/data/dataset_inteli_crop/newTargetMasks2'\n",
        "\n",
        "# Processar imagens e máscaras em batches\n",
        "all_images = []\n",
        "all_masks = []\n",
        "batch_size = 51\n",
        "for batch_images, batch_masks in load_images_and_masks_in_batches(data_dir, mask_dir, batch_size):\n",
        "    all_images.extend(batch_images)\n",
        "    all_masks.extend(batch_masks)\n",
        "\n",
        "# Dividir os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_images, all_masks, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq21Z5gPNLXE"
      },
      "source": [
        "## Função de perda personalizada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFAwOFAgNM2C"
      },
      "source": [
        "Definimos uma função de perda personalizada que combina a perda Dice e a perda binária de entropia cruzada para otimizar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88A_wQBr1uWZ"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCVCgzJbNQMo"
      },
      "source": [
        "## Arquitetura do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG-nM2G4ShEn"
      },
      "source": [
        "### Descrição Do Modelo Utilizado - Estrutura Geral\n",
        "\n",
        "O modelo U-Net é uma rede neural convolucional projetada para tarefas de segmentação de imagem. A arquitetura é composta por duas partes principais: o codificador (encoder) e o decodificador (decoder). A estrutura do U-Net permite capturar tanto o contexto global quanto os detalhes locais da imagem. O U-Net, proposto por Ronneberger et al. (2015), destaca-se pelo uso de conexões de skip entre o encoder e o decoder, que preservam informações de alta resolução essenciais para segmentação precisa. A extensão 3D U-Net, proposta por Çiçek et al. (2016), melhora ainda mais a capacidade do modelo em aplicações volumétricas. Ele é amplamente utilizado em diversas aplicações, incluindo a segmentação de tumores e análise de imagens médicas.\n",
        "\n",
        "Referências:\n",
        "\n",
        "Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 (pp. 234-241).\n",
        "\n",
        "Çiçek, Ö., et al. (2016). 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016 (pp. 424-432).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8uxCnb-SjOl"
      },
      "source": [
        "### Codificador (Encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAdjAsIjSrJf"
      },
      "source": [
        "\n",
        "O codificador é baseado na arquitetura VGG16 pré-treinada no conjunto de dados ImageNet. Ele consiste em uma série de camadas convolucionais seguidas por camadas de pooling. A função do codificador é extrair características da imagem de entrada em diferentes níveis de abstração. As camadas convolucionais e de pooling são:\n",
        "\n",
        "\n",
        "- **Block 1:**\n",
        "  - 2 camadas Conv2D com 64 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 2:**\n",
        "  - 2 camadas Conv2D com 128 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 3:**\n",
        "  - 3 camadas Conv2D com 256 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 4:**\n",
        "  - 3 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU\n",
        "  - 1 camada MaxPooling2D com tamanho de pool 2x2\n",
        "\n",
        "- **Block 5:**\n",
        "  - 3 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMYx7ILYSs-X"
      },
      "source": [
        "### Decodificador (Decoder)\n",
        "O decodificador reconstrói a imagem de saída a partir das características extraídas pelo codificador. Utiliza operações de upsampling e camadas convolucionais para aumentar a resolução da imagem. As camadas do decodificador incluem:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTxvbbcJStoe"
      },
      "source": [
        "- **UpSampling 1:**\n",
        "  - 1 camada Conv2DTranspose com 512 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 512 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 2:**\n",
        "  - 1 camada Conv2DTranspose com 256 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 256 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 3:**\n",
        "  - 1 camada Conv2DTranspose com 128 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 128 filtros e kernel 3x3, ativação ReLU\n",
        "\n",
        "- **UpSampling 4:**\n",
        "  - 1 camada Conv2DTranspose com 64 filtros e kernel 2x2\n",
        "  - 2 camadas Conv2D com 64 filtros e kernel 3x3, ativação ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HbkNxL9SwhP"
      },
      "source": [
        "\n",
        "### Saída (Output)\n",
        "A última camada do decodificador é uma camada Conv2D com um único filtro e função de ativação sigmoid, produzindo a máscara segmentada binária."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6P2ihn0vWzx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, concatenate, Cropping2D, ZeroPadding2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def unet_with_vgg16_backbone(input_size=(120, 120, 3)):\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "\n",
        "    # Freeze the VGG16 layers\n",
        "    for layer in vgg16.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Encoder\n",
        "    s1 = vgg16.get_layer(\"block1_conv2\").output\n",
        "    s2 = vgg16.get_layer(\"block2_conv2\").output\n",
        "    s3 = vgg16.get_layer(\"block3_conv3\").output\n",
        "    s4 = vgg16.get_layer(\"block4_conv3\").output\n",
        "    b1 = vgg16.get_layer(\"block5_conv3\").output\n",
        "\n",
        "    # Decoder\n",
        "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(b1)\n",
        "    if u6.shape[1] != s4.shape[1] or u6.shape[2] != s4.shape[2]:\n",
        "        u6 = ZeroPadding2D(((0, s4.shape[1] - u6.shape[1]), (0, s4.shape[2] - u6.shape[2])))(u6)\n",
        "    u6 = concatenate([u6, s4])\n",
        "\n",
        "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(u6)\n",
        "    if u7.shape[1] != s3.shape[1] or u7.shape[2] != s3.shape[2]:\n",
        "        u7 = ZeroPadding2D(((0, s3.shape[1] - u7.shape[1]), (0, s3.shape[2] - u7.shape[2])))(u7)\n",
        "    u7 = concatenate([u7, s3])\n",
        "\n",
        "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(u7)\n",
        "    if u8.shape[1] != s2.shape[1] or u8.shape[2] != s2.shape[2]:\n",
        "        u8 = ZeroPadding2D(((0, s2.shape[1] - u8.shape[1]), (0, s2.shape[2] - u8.shape[2])))(u8)\n",
        "    u8 = concatenate([u8, s2])\n",
        "\n",
        "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u8)\n",
        "    if u9.shape[1] != s1.shape[1] or u9.shape[2] != s1.shape[2]:\n",
        "        u9 = ZeroPadding2D(((0, s1.shape[1] - u9.shape[1]), (0, s1.shape[2] - u9.shape[2])))(u9)\n",
        "    u9 = concatenate([u9, s1])\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(u9)\n",
        "\n",
        "    model = Model(inputs=vgg16.input, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Criar o modelo\n",
        "model = unet_with_vgg16_backbone(input_size=(120, 120, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOTZU9heNWEx"
      },
      "source": [
        "## Divisão dos Dados e Treinamento\n",
        "Dividimos os dados em conjuntos de treino e teste e treinamos o modelo U-Net com VGG16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBevdlUGYmIu"
      },
      "outputs": [],
      "source": [
        "# Dividir os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju58IauNxKWL"
      },
      "outputs": [],
      "source": [
        "# Criar o modelo\n",
        "model = unet_with_vgg16_backbone(input_size=(120, 120, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L2P3jrQxO2u",
        "outputId": "c2bf2ad4-0caa-4751-d215-b97f5d55d84e"
      },
      "outputs": [],
      "source": [
        "# Treinar o modelo\n",
        "results = model.fit(X_train, y_train, validation_split=0.1, batch_size=1, epochs=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oHXfUd91xUQj",
        "outputId": "e34b6717-ee6f-411f-dad9-5f90ac241406"
      },
      "outputs": [],
      "source": [
        "# Avaliar o modelo\n",
        "model.evaluate(X_test, y_test, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhoCwWThNqvF"
      },
      "source": [
        "## Plotando métricas\n",
        "Plotamos as métricas de perda e acurácia do modelo durante o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "t_Ee08KIKwot",
        "outputId": "c397c7a4-0e4a-48c5-e0ba-5f1dea4e9bb2"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(history):\n",
        "    # Plotando a perda (loss)\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Validação')\n",
        "    plt.title('Perda')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando a acurácia (accuracy)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validação')\n",
        "    plt.title('Acurácia')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.xlabel('Época')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK-BDvBlNwXp"
      },
      "source": [
        "## Plotando resultados\n",
        "Plotamos as imagens de entrada, as máscaras reais e as máscaras preditas pelo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiZxl2sn0vlk"
      },
      "outputs": [],
      "source": [
        "# Função para plotar os resultados\n",
        "def plot_results(X, y, model, ix=None):\n",
        "    \"\"\"Plotar a imagem, a máscara real e a máscara predita.\n",
        "\n",
        "    Args:\n",
        "    - X: array de imagens de entrada.\n",
        "    - y: array de máscaras reais.\n",
        "    - model: modelo treinado U-Net.\n",
        "    - ix: índice da imagem a ser plotada. Se None, seleciona uma imagem aleatoriamente.\n",
        "    \"\"\"\n",
        "    if ix is None:\n",
        "        ix = np.random.randint(0, len(X))\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
        "    ax[0].imshow(X[ix], cmap='gray')\n",
        "    ax[0].title.set_text('Imagem Original')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(y[ix].squeeze(), cmap='gray')  # Squeeze para remover dimensões extras se houver\n",
        "    ax[1].title.set_text('Máscara Real')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    # Fazer a predição usando o modelo\n",
        "    pred = model.predict(X[ix:ix+1])\n",
        "    pred = (pred > 0.5).astype(np.float32)  # Binarizar a predição\n",
        "\n",
        "    ax[2].imshow(pred.squeeze(), cmap='gray')  # Squeeze para garantir que está em 2D\n",
        "    ax[2].title.set_text('Máscara Predita')\n",
        "    ax[2].axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "lakQgC2a2tqy",
        "outputId": "54350fc5-cb7d-4955-c307-d66ee2bbf389"
      },
      "outputs": [],
      "source": [
        "index = None\n",
        "plot_results(X_test, y_test, model, ix=index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L1XU9wMQhJm"
      },
      "source": [
        "## Instruções para Obtenção do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2x93e1RHtG"
      },
      "source": [
        "### Passo 1: Aquisição dos Dados\n",
        "Os dados utilizados neste exemplo são imagens e máscaras que podem ser carregadas a partir de um diretório local ou de uma unidade do Google Drive. Certifique-se de que as imagens e as máscaras estejam no formato PNG e que as máscaras sejam imagens em escala de cinza (grayscale).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzS5_oPKRLlI"
      },
      "source": [
        "### Passo 2: Configuração do Ambiente\n",
        "Para utilizar a GPU no Google Colab:\n",
        "\n",
        "1. Vá para `Tempo de execução` ou `Runtime` no menu.\n",
        "2. Selecione `Alterar tipo de tempo de execução` ou `Change runtime`.\n",
        "3. Na janela que abrir, escolha `GPU` no menu suspenso `Acelerador de hardware`, recomendamos o uso da A100 pela capacidade mista de CPU e GPU.\n",
        "\n",
        "Após realizar esses passos, a GPU será habilitada para o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz4h5XbuRRRm"
      },
      "source": [
        "### Passo 3: Preparação dos Dados\n",
        "Os dados devem ser normalizados e divididos em conjuntos de treino e teste. As imagens são normalizadas para o intervalo [0, 1], e as máscaras são binarizadas (valores acima de 0.5 são considerados 1 e abaixo são 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C9mEoQzRTM3"
      },
      "source": [
        "### Passo 4: Treinamento do Modelo\n",
        "O modelo U-Net com backbone VGG16 é treinado utilizando os dados preparados. Ajuste os hiperparâmetros como número de épocas, tamanho do lote e taxa de aprendizado conforme necessário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apMd2GleRVS3"
      },
      "source": [
        "### Passo 5: Avaliação do Modelo\n",
        "Após o treinamento, o modelo é avaliado no conjunto de teste para medir seu desempenho. Métricas como a perda e a acurácia são plotadas para análise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIipLNtORXd1"
      },
      "source": [
        "\n",
        "### Passo 6: Refinamento do Modelo\n",
        "Se o desempenho do modelo não for satisfatório, existem as seguintes estratégias de refinamento:\n",
        "1. **Ajuste de Hiperparâmetros:** Modificar a taxa de aprendizado (learning rate ou `lr`), o número de épocas (`epochs`), o tamanho do lote (`batch_size`), etc.\n",
        "2. **Aumento de Dados (Data Augmentation):** Utilize técnicas de aumento de dados para gerar mais exemplos de treino, como rotações, zoom, translações, etc.\n",
        "3. **Arquitetura do Modelo:** Modificar a arquitetura do modelo, adicionando mais camadas ou unidades.\n",
        "4. **Regularização:** Adicionar camadas de Dropout ou ajuste a regularização L2 para evitar overfitting.\n",
        "5. **Treinamento com Mais Dados:** Adicionar mais dados de treino para melhorar a generalização do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWuxBRdjVHX1"
      },
      "source": [
        "# Instruções para deploy em um serviço em nuvem\n",
        "\n",
        "As seguintes instruções fornecem um guia para preparar, fazer upload e fazer o deploy do modelo de segmentação no AWS SageMaker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWkVb-N8VREL"
      },
      "source": [
        "### 1. Preparação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3NcQ-b2VUZB"
      },
      "source": [
        "Instale a AWS CLI e configure suas credenciais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gtlogVwVXf1"
      },
      "outputs": [],
      "source": [
        "pip install awscli\n",
        "aws configure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXLEP-PnVb90"
      },
      "source": [
        "Instale o SDK do SageMaker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYbsd_LIVeQy"
      },
      "outputs": [],
      "source": [
        "pip install sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHHcJiCbVpeJ"
      },
      "source": [
        "### 2. Preparar o notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuy83-awVsgB"
      },
      "source": [
        "Certifique-se que todas as dependências do notebook estão instaladas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ7TMYdcVwDv"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas scikit-learn matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOEPgPZtVy99"
      },
      "source": [
        "### 3. Preparação do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqk3zq_-V1uI"
      },
      "source": [
        "Após o treinamento do modelo, salve-o no formato .h5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6P1BlSfV4x1"
      },
      "outputs": [],
      "source": [
        "model.save('segmentation_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFU67brCV_Tf"
      },
      "source": [
        "Após isso, carregue o arquivo do modelo salvo para um bucket no S3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTSlWjGkV_23"
      },
      "outputs": [],
      "source": [
        "aws s3 cp segmentation_model.h5 s3://nome-do-bucket/modelos/segmentation_model.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52a6LCNYWCVT"
      },
      "source": [
        "### 4. Deploy no SageMaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aXrl4h1WEfs"
      },
      "source": [
        "Crie um arquivo inference.py e coloque o seguinte código que contém a lógica de carregamento do modelo e inferência:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHAsTyUiWHrO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    model = load_model(f'{model_dir}/segmentation_model.h5')\n",
        "    return model\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    if request_content_type == 'application/json':\n",
        "        return np.array(json.loads(request_body)['instances'])\n",
        "    else:\n",
        "        raise ValueError('Esse modelo apenas suporta JSON')\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    predictions = model.predict(input_data)\n",
        "    return predictions\n",
        "\n",
        "def output_fn(prediction, response_content_type):\n",
        "    return json.dumps({'predições': prediction.tolist()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PnqpnU6WLcA"
      },
      "source": [
        "Utilize o SDK do SageMaker para criar um endpoint para o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUd4lip4WOqw"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "from sagemaker.tensorflow import TensorFlowModel\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = 'arn:aws:iam::sua-conta:permissao/sagemaker-permissao'\n",
        "\n",
        "model = TensorFlowModel(model_data='s3://nome-bucket/modelos/segmentation_model.h5',\n",
        "                        role=role,\n",
        "                        entry_point='inference.py',\n",
        "                        framework_version='2.3',\n",
        "                        sagemaker_session=sagemaker_session)\n",
        "\n",
        "predictor = model.deploy(initial_instance_count=1,\n",
        "                         instance_type='ml.m4.xlarge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0VX5XaxWTg7"
      },
      "source": [
        "### 5. Teste e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlq8_fsjWWvE"
      },
      "source": [
        "Envie uma requisição de teste ao endpoint para garantir que tudo esteja funcionando corretamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmCcROXTWaev"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import boto3\n",
        "\n",
        "runtime = boto3.client('runtime.sagemaker')\n",
        "payload = json.dumps({'instances': np.random.rand(1, 224, 224, 3).tolist()})\n",
        "\n",
        "response = runtime.invoke_endpoint(EndpointName=predictor.endpoint_name,\n",
        "                                   ContentType='application/json',\n",
        "                                   Body=payload)\n",
        "\n",
        "result = json.loads(response['Body'].read().decode())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAf8p-c-Whcn"
      },
      "source": [
        "### 6. Limpeza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlN_HqhTWj-A"
      },
      "source": [
        "Para evitar custos desnecessários, remova os recursos quando terminar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDKvFD0nWrKx"
      },
      "outputs": [],
      "source": [
        "predictor.delete_endpoint()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "I0GgLVB6fI_T"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
